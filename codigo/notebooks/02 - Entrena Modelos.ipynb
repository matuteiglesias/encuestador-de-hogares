{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos encuestadores de hogares EPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a script de python con nbconvert, \n",
    "# removiendo las celdas que estan pensadas para la notebook interactiva, no para el script.\n",
    "# De esta forma no vamos a incluir las celdas de input, \n",
    "# \n",
    "\n",
    "# jupyter nbconvert --to python --TagRemovePreprocessor.remove_cell_tags \"interactive-only\"  02\\ -\\ Train\\ ML\\ algos\\ on\\ EPH.ipynb\n",
    "\n",
    "## Funciona en las versiones mas nuevas de jupyter, en caso que nbconvert de error, verificar:\n",
    "# jupyter --version\n",
    "\n",
    "## Correr:\n",
    "# conda update jupyter\n",
    "\n",
    "# Y volver a intentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seteo de Parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTOS INTRODUCIDOS POR EL USUARIO\n",
      "Start year:  2023\n",
      "End year:  2025\n",
      "Overwrite:  True\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "if get_ipython() is None:\n",
    "    print('ARGUMENTOS TOMADOS DE CLI')\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='A script to process data for a range of years')\n",
    "\n",
    "    parser.add_argument('-y','--years', nargs='+', help='Set the range of years to process data for. Default is the current year and the next year', required=False, type=int, default=[2022, 2023])\n",
    "    parser.add_argument('-ow','--overwrite', nargs=1, required=False, default= True, help='Flag to specify if previous data should be overwritten. Default is True')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    overwrite = args.overwrite\n",
    "    startyr = args.years[0]\n",
    "    endyr = args.years[1]\n",
    "    \n",
    "else:\n",
    "    print('ARGUMENTOS INTRODUCIDOS POR EL USUARIO')\n",
    "    startyr = input(\"Enter the start year [default: 2022]: \") or 2022\n",
    "    endyr = input(\"Enter the end year [default: 2023]: \") or 2023\n",
    "    overwrite = input(\"Do you want to overwrite previous data? [y/n] [default: y]: \") or \"y\"\n",
    "\n",
    "    if overwrite.lower() == \"y\":\n",
    "        overwrite = True\n",
    "    else:\n",
    "        overwrite = False\n",
    "\n",
    "    #Convert the input to integers\n",
    "    startyr = int(startyr)\n",
    "    endyr = int(endyr)\n",
    "\n",
    "    print(\"Start year: \", startyr)\n",
    "    print(\"End year: \", endyr)\n",
    "    print(\"Overwrite: \", overwrite)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names\n",
    "# y_cols = ['CAT_OCUP', 'P47T', 'PP10E', 'PP10D', 'PP07K', 'PP07I', 'V3_M', 'PP07G4', 'CH16', 'T_VI', \n",
    "#           'V12_M', 'TOT_P12', 'PP07G3', 'V5_M', 'PP07H', 'V2_M', 'PP10C', \n",
    "#           'PP08D1', 'PP07J', 'CAT_INAC', 'CH07', 'CH08', 'P21', 'PP07G1', 'PP07G_59', 'PP07G2']\n",
    "\n",
    "# quitando las columns ['PP10E', 'PP10D', 'CH16', 'PP10C', 'CH08'] (changas, hace cuanto no trabaja, se mudo en 5 anios, nunca trabajo_)\n",
    "# y_cols = ['CAT_OCUP', 'P47T', 'PP07K', 'PP07I', 'V3_M', 'PP07G4', 'T_VI', \n",
    "#           'V12_M', 'TOT_P12', 'PP07G3', 'V5_M', 'PP07H', 'V2_M', \n",
    "#           'PP08D1', 'PP07J', 'CAT_INAC', 'CH07', 'P21', 'PP07G1', 'PP07G_59', 'PP07G2']\n",
    "\n",
    "x_cols1 = ['IX_TOT', 'P02', 'P03', 'AGLO_rk', 'Reg_rk', 'V01', 'H05', 'H06',\n",
    "       'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14',\n",
    "       'H13', 'P07', 'P08', 'P09', 'P10', 'P05', 'CONDACT']\n",
    "\n",
    "predecir1 = ['CAT_OCUP', 'CAT_INAC', 'CH07']\n",
    "\n",
    "x_cols2 = x_cols1 + predecir1\n",
    "predecir2 = ['INGRESO', 'INGRESO_NLB', 'INGRESO_JUB', 'INGRESO_SBS']\n",
    "\n",
    "x_cols3 = x_cols2 + predecir2\n",
    "# La seccion PP07G pregunta si el trabajo es en blanco y que beneficios tiene. Puede ayudar a la regresion para ingresos.\n",
    "# predecir3 = ['PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K']\n",
    "predecir3 = ['PP07G1','PP07G_59', 'PP07I', 'PP07J', 'PP07K']\n",
    "\n",
    "# Columnas de ingresos. Necesitan una regresion...\n",
    "columnas_pesos = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n",
    "x_cols4 = x_cols3 + predecir3\n",
    "# Columnas de ingresos. Necesitan una regresion..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones clasificador y regresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save models at: (ocupan bastante memoria en disco)\n",
    "# models_path = '/media/miglesia/Elements/suite/encuestador-de-hogares'\n",
    "models_path = './..'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si el modulo sklearn no esta instalado, lo instalo\n",
    "try:\n",
    "    import sklearn\n",
    "except ImportError:\n",
    "    !pip install sklearn\n",
    "    import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # importamos las funciones para dividir los datos\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor # importamos los modelos de random forest\n",
    "import os # para trabajar con el sistema operativo\n",
    "import joblib # para guardar el modelo entrenado\n",
    "\n",
    "def fit_model(train_data, x_cols, y_cols, out_filename, model):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de random forest.\n",
    "    train_data: dataframe con los datos de entrenamiento\n",
    "    x_cols: lista con los nombres de las columnas de entrada del modelo\n",
    "    y_cols: lista con los nombres de las columnas de salida del modelo\n",
    "    out_filename: string con la ruta y nombre del archivo donde se guardarÃ¡ el modelo\n",
    "    model: instancia del modelo a entrenar\n",
    "    \"\"\"\n",
    "    X = train_data[x_cols] # separamos las columnas de entrada\n",
    "    y = train_data[y_cols] # separamos las columnas de salida\n",
    "    \n",
    "    X, X_test, y, y_test = train_test_split(X, y, test_size=0.1) # dividimos los datos en entrenamiento y test\n",
    "    \n",
    "    clf = model.fit(X.values, y.values) # entrenamos el modelo\n",
    "    \n",
    "    # guardamos el modelo en disco\n",
    "    if not os.path.exists(models_path + '/fitted_RF/'):\n",
    "        os.makedirs(models_path + '/fitted_RF/')\n",
    "    joblib.dump(model, out_filename, compress=3)\n",
    "    print('saved model at: ' + out_filename)\n",
    "\n",
    "\n",
    "\n",
    "#     print(sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)[:5])\n",
    "    del clf\n",
    "    del X; del y # liberar memoria eliminando los dataframes mas pesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_csv('./../data/training/EPHARG_train_23.csv')\n",
    "\n",
    "# train_q = train_data.loc[train_data.Q == q]\n",
    "# # fit_model(train_q, x_cols = x_cols4, y_cols = y_cols4, out_filename = out,\n",
    "# #         model = RandomForestRegressor(n_estimators=1, max_depth = 20, n_jobs = -1))\n",
    "\n",
    "# model = RandomForestRegressor(n_estimators=1, max_depth = 20, n_jobs = -1)\n",
    "# y_cols = columnas_pesos\n",
    "# x_cols = x_cols4\n",
    "\n",
    "# X = train_data[x_cols] # separamos las columnas de entrada\n",
    "# y = train_data[y_cols] # separamos las columnas de salida\n",
    "\n",
    "# X, X_test, y, y_test = train_test_split(X, y, test_size=0.1) # dividimos los datos en entrenamiento y test\n",
    "\n",
    "# clf = model.fit(X.values, y.values) # entrenamos el modelo\n",
    "\n",
    "# # guardamos el modelo en disco\n",
    "# if not os.path.exists(models_path + '/fitted_RF/'):\n",
    "#     os.makedirs(models_path + '/fitted_RF/')\n",
    "\n",
    "# # out_filename = models_path + '/fitted_RF/' + out + '.joblib'\n",
    "# out_filename = './../fitted_RF/clf4_2023-02-15_ARG'\n",
    "# joblib.dump(model, out_filename, compress=3)\n",
    "# print('saved model at: ' + out_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop principal. Entrenar y guardar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')\n",
    "\n",
    "# './../data/training/EPHARG_train_'+yr[2:]+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Entrenar modelo, para cada trimestre\n",
    "# for q in train_data.Q.unique():\n",
    "#     print(q)\n",
    "#     out = models_path + '/fitted_RF/clf4_'+q+'_ARG'\n",
    "#     if (not os.path.exists(out)) or (overwrite):\n",
    "#         train_q = train_data.loc[train_data.Q == q]\n",
    "#         fit_model(train_q, x_cols = x_cols4, y_cols = columnas_pesos, out_filename = out,\n",
    "#                     model = RandomForestRegressor(n_estimators=1, max_depth = 20, n_jobs = -1))\n",
    "#         del train_q;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m out \u001b[38;5;241m=\u001b[39m models_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/fitted_RF/clf1_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39myr\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ARG\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(out)) \u001b[38;5;129;01mor\u001b[39;00m (overwrite):\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_cols1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_cols\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpredecir1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_filename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m             \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m## ETAPA 2:\u001b[39;00m\n\u001b[1;32m     12\u001b[0m out \u001b[38;5;241m=\u001b[39m models_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/fitted_RF/clf2_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39myr\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_ARG\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[37], line 20\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(train_data, x_cols, y_cols, out_filename, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m y \u001b[38;5;241m=\u001b[39m train_data[y_cols] \u001b[38;5;66;03m# separamos las columnas de salida\u001b[39;00m\n\u001b[1;32m     18\u001b[0m X, X_test, y, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;66;03m# dividimos los datos en entrenamiento y test\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# entrenamos el modelo\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# guardamos el modelo en disco\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(models_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/fitted_RF/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/joblib/parallel.py:1944\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/joblib/parallel.py:1587\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1586\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1587\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1593\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/base2/lib/python3.11/site-packages/joblib/parallel.py:1699\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1698\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1699\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "    print(yr)\n",
    "    train_data = pd.read_csv('./../../data/training/EPHARG_train_'+yr[2:]+'.csv')\n",
    "    \n",
    "    ## ETAPA 1:\n",
    "    out = models_path + '/fitted_RF/clf1_'+yr+'_ARG'\n",
    "    if (not os.path.exists(out)) or (overwrite):\n",
    "        fit_model(train_data, x_cols = x_cols1, y_cols = predecir1, out_filename = out,\n",
    "                 model = RandomForestClassifier(n_estimators=100, max_depth = 15, n_jobs = -1))\n",
    "    \n",
    "    ## ETAPA 2:\n",
    "    out = models_path + '/fitted_RF/clf2_'+yr+'_ARG'\n",
    "    if (not os.path.exists(out)) or (overwrite):\n",
    "        fit_model(train_data, x_cols = x_cols2, y_cols = predecir2, out_filename = out,\n",
    "                 model = RandomForestClassifier(n_estimators=100, max_depth = 15, n_jobs = -1))\n",
    "    \n",
    "    ## ETAPA 3:\n",
    "    out = models_path + '/fitted_RF/clf3_'+yr+'_ARG'\n",
    "    if (not os.path.exists(out)) or (overwrite):\n",
    "        fit_model(train_data, x_cols = x_cols3, y_cols = predecir3, out_filename = out,\n",
    "                 model = RandomForestClassifier(n_estimators=100, max_depth = 15, n_jobs = -1))\n",
    "    \n",
    "    ## ETAPA 4 (Regresion)\n",
    "    ## Tomar log de las columnas en pesos.\n",
    "    train_data[columnas_pesos] = log10(train_data[columnas_pesos].clip(-.9) + 1)\n",
    "\n",
    "    ## Entrenar modelo, para cada trimestre\n",
    "    for q in train_data.Q.unique():\n",
    "        print(q)\n",
    "        out = models_path + '/fitted_RF/clf4_'+q+'_ARG'\n",
    "        if (not os.path.exists(out)) or (overwrite):\n",
    "            train_q = train_data.loc[train_data.Q == q]\n",
    "            fit_model(train_q, x_cols = x_cols4, y_cols = columnas_pesos, out_filename = out,\n",
    "                     model = RandomForestRegressor(n_estimators=1, max_depth = 20, n_jobs = -1))\n",
    "            del train_q;\n",
    "\n",
    "    del train_data; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
