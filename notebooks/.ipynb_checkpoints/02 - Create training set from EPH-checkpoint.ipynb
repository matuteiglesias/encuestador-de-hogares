{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads EPH data, cleans it and arranges it to be used as 'training sets'. \n",
    "That is, for fitting any Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startyr = 2020\n",
    "endyr = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "# radio_ref[['PROV','NOMPROV','DPTO', 'NOMDPTO']].drop_duplicates().to_csv('./../data/DPTO_PROV.csv', index = False)\n",
    "dpto_region = pd.read_csv('./../data/info/DPTO_PROV_Region.csv')\n",
    "radio_ref = radio_ref.merge(dpto_region)\n",
    "AGLO_Region = radio_ref[['AGLOMERADO', 'Region']].drop_duplicates()\n",
    "\n",
    "# Decision sobre cual es la region de un aglomerado. GBA tiene que ir a Gran Buenos Aires, aunque algunos de sus radios en partidos como Rodriguez, Escobar, etc sean region pampeana.\n",
    "# Viedma Patagones, se tendria que tirar de un lado, y la mayoria de sus radios, son Patagonia.\n",
    "# Se tiene que corregir a mano, porque el AGLO 0 SI tiene varias regiones.\n",
    "\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 33) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 93) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "\n",
    "### Match column names\n",
    "\n",
    "names_censo = ['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO',\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']\n",
    "\n",
    "\n",
    "names_EPH = ['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']\n",
    "\n",
    "col_mon = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi = pd.read_csv('./../data/info/indice_precios_Q.csv', index_col=0)\n",
    "\n",
    "cpi.index = pd.to_datetime(cpi.index)\n",
    "cpi = cpi['2003':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>3188.361836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-31</th>\n",
       "      <td>3270.799721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  index\n",
       "2020-12-31  3188.361836\n",
       "2021-01-31  3270.799721"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpi_M = pd.read_csv('./../data/info/indice_precios_M.csv', index_col=0)\n",
    "cpi_mes_actual = cpi_M.iloc[-1][0]\n",
    "\n",
    "cpi_M.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3270.7997208542934"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpi_mes_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tseries.offsets import MonthEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPHs\n",
    "\n",
    "Los microdatos de la Encuesta Permanente de Hogares se pueden descargar con:\n",
    "\n",
    "``git clone https://github.com/matuteiglesias/microdatos-EPH-INDEC.git``\n",
    "\n",
    "(darle star al repositorio)\n",
    "\n",
    "tomando pull del mismo repositorio se va a poder actualizar con los nuevos microdatos a medida que se publican. Siempre y cuando el repositorio se mantenga actualizado. Y hasta que no se supere la capacidad de almacenamiento en repo github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "16845\n",
      "11976\n",
      "(28799, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t120.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t220.txt\n",
      "(88613, 33)\n",
      "Hogar - Indiv merged:\n",
      "(90788, 49)\n",
      "No aglo agregado:\n",
      "(181576, 50)\n",
      "deflactado:\n",
      "(181576, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21        11228.098581\n",
       "P47T       19412.007071\n",
       "PP08D1     23814.104135\n",
       "TOT_P12      783.981132\n",
       "T_VI        6828.807386\n",
       "V12_M        374.599330\n",
       "V2_M        4507.146583\n",
       "V3_M         184.668315\n",
       "V5_M         732.604023\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path ='./../../microdatos-EPH-INDEC/microdatos/' # depende de donde hayamos descargado los microdatos\n",
    "# path ='./../../EPH/microdatos/' # depende de donde hayamos descargado los microdatos\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    allFiles = glob.glob(path + 'hogar/*'+str(yr)+'.txt')\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                        usecols = ['CODUSU','ANO4','TRIMESTRE','IX_TOT', 'AGLOMERADO',\n",
    "        'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9']) \n",
    "        ['II2', 'IV5', 'IX_TOT', 'II7', 'IV4', 'II1', 'IV7', 'IV6', 'IV11', 'IV8', 'IV3', 'II8', 'IV1', 'IV10', 'II9']\n",
    "        \n",
    "        print(len(df))\n",
    "        list_ += [df]\n",
    "    df = pd.concat(list_)\n",
    "\n",
    "    # Correcciones Respuestas. Para que matchee censo\n",
    "    df = df.loc[df.IV1 != 9]\n",
    "    df['IV10'] = df['IV10'].map({1: 1, 2: 2, 3: 2, 0: 0, 9: 9})\n",
    "    df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "    df['II7'] = df['II7'].map({1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 6, 8: 6, 9: 6, 0: 0})\n",
    "    df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "    df['IX_TOT'] = df['IX_TOT'].clip(0, 8)\n",
    "    \n",
    "    hogar = df\n",
    "    hogar = hogar.drop_duplicates()\n",
    "    print(hogar.shape)\n",
    "\n",
    "    allFiles = glob.glob(path + 'individual/usu_individual*'+str(yr)+'.txt')\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in allFiles:\n",
    "        print(file_)\n",
    "    #     print(file_)\n",
    "        df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                         usecols = ['CODUSU','ANO4','TRIMESTRE','CH04','CH06', 'AGLOMERADO', 'CH09','CH10','CH12','CH13','CH15'] +\\\n",
    "                         ['CH07', 'ESTADO','CAT_INAC','CAT_OCUP','PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K',\n",
    "                         'P47T', 'V3_M', 'T_VI', 'V12_M', 'TOT_P12', 'V5_M','V2_M', 'PP08D1', 'P21'])\n",
    "        df = df.rename(columns = {'ESTADO': 'CONDACT'})\n",
    "\n",
    "#         display(df.head())\n",
    "# revisar estado, condact, cat ocup, cat inac.\n",
    "    # For the regression training set. But for these the ANO4 TRIMESTRE is important.. Also we need more memory.\n",
    "    #                      ['P21','P47T',,'CH08','CH16','TOT_P12','T_VI','V10_M','V11_M','V12_M','V18_M','V19_AM','V21_M','V2_M','V3_M',\n",
    "    #             'V4_M','V5_M','V8_M','V9_M','PP08D1','PP08D4','PP08F1','PP08F2','PP08J1','PP08J2','PP08J3','PP10A','PP10C','PP10D','PP10E'])\n",
    "#         print(len(df))\n",
    "        list_ += [df]\n",
    "    df = pd.concat(list_)\n",
    "\n",
    "    # Correcciones Respuestas. Para que matchee censo\n",
    "    df['CH15'] = df['CH15'].map({1:1, 2:1, 3:1, 4:2, 5:2, 9:0})\n",
    "    df['CH06'] = df['CH06'].clip(0)\n",
    "    df['CH09'] = df['CH09'].map({1:1, 2:2, 0:2, 3:2})\n",
    "    df.loc[df['CH06'] < 14, 'CONDACT'] = 0 # Menores de 14 van con CONDACT 0, como en el Censo\n",
    "    \n",
    "    ## En Censo, Jardin y educacion especial no preguntan terminado si/no.\n",
    "    df['CH12'] = df.CH12.replace(99, 0)\n",
    "    df.loc[df.CH12.isin([0, 1, 9]), 'CH13'] = 0\n",
    "\n",
    "#     df['MAYOR'] = df['CH06'] >= 14 \n",
    "#     df['MAYOR'] = df['CH06'] // 7\n",
    "#     df['CONDACT'] = df['CAT_OCUP'].fillna(-1)\n",
    "\n",
    "    indiv = df\n",
    "    indiv = indiv.dropna(subset = ['P47T'])\n",
    "    print(indiv.shape)\n",
    "\n",
    "    indiv_table = indiv[list(indiv.columns.difference(hogar.columns)) + ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO']]\n",
    "    EPH = hogar.merge(indiv_table, on = ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO'], indicator = True)\n",
    "\n",
    "    print('Hogar - Indiv merged:')\n",
    "    print(EPH.shape)\n",
    "\n",
    "    \n",
    "#     EPH = EPH.loc[EPH.P47T != -9]\n",
    "    \n",
    "    EPH = EPH.merge(AGLO_Region)\n",
    "\n",
    "    EPH_no_aglo = EPH.copy(); \n",
    "    EPH_no_aglo['AGLOMERADO'] = 0\n",
    "\n",
    "    EPH = pd.concat([EPH, EPH_no_aglo]).reset_index(drop = True)\n",
    "\n",
    "    print('No aglo agregado:')\n",
    "    print(EPH.shape)\n",
    "    \n",
    "    # Quarters / deflation\n",
    "    EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "    EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') + MonthEnd(1)\n",
    "#     cpi_ultimo_Q = indice_precios['index'].values[-1]\n",
    "    \n",
    "    EPH[col_mon] = cpi_mes_actual*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "    \n",
    "    # 2018Q3 -> Mar19 1.3156\n",
    "    # 2018Q3 -> Abr19 1.361\n",
    "#     EPH[col_mon] = 1.361*EPH[col_mon]\n",
    "    \n",
    "    EPH[col_mon] = EPH[col_mon].round()\n",
    "    \n",
    "    print('deflactado:')\n",
    "    print(EPH.shape)\n",
    "    display(EPH[col_mon].mean())\n",
    "    \n",
    "    \n",
    "    training = EPH.rename(columns = dict(zip(names_EPH, names_censo)))\n",
    "    \n",
    "    if not os.path.exists('./../data/training/'):\n",
    "        os.makedirs('./../data/training/')\n",
    "    \n",
    "    training.to_csv('./../data/training/EPHARG_train_'+str(yr)+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de AGLOS y Regiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "# for yr in [str(s) for s in [2006, 2011, 2016]]:\n",
    "# for yr in [str(s) for s in [2015, 2020]]:\n",
    "for yr in [str(s) for s in [2020]]:  # Esto depende de los anios en que tengamos la microdata, podemos elegir\n",
    "# for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "    print(yr)\n",
    "    train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')\n",
    "    train = train.loc[train.P47T >= -0.001].fillna(0)#.sample(400000)\n",
    "    df_list += [train]\n",
    "    \n",
    "train_df = pd.concat(df_list)\n",
    "\n",
    "AGLO_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['AGLOMERADO'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'AGLO_rk'})\n",
    "Reg_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['Region'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'Reg_rk'})\n",
    "\n",
    "AGLO_rk['AGLO_rk'] = AGLO_rk.AGLO_rk/AGLO_rk.AGLO_rk.max()\n",
    "AGLO_rk.to_csv('./../data/info/AGLO_rk', index = False)\n",
    "Reg_rk['Reg_rk'] = Reg_rk.Reg_rk/Reg_rk.Reg_rk.max()\n",
    "Reg_rk.to_csv('./../data/info/Reg_rk', index = False)\n",
    "\n",
    "# check it out\n",
    "# AGLO_rk.merge(pd.read_csv('./../data/info/aglo_labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    }
   ],
   "source": [
    "AGLO_rk = pd.read_csv('./../data/info/AGLO_rk')\n",
    "Reg_rk = pd.read_csv('./../data/info/Reg_rk')\n",
    "\n",
    "df_list = []\n",
    "for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "# for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "    print(yr)\n",
    "    train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')#.drop(['AGLO_rk', 'Reg_rk'], axis = 1)\n",
    "    train = train.loc[train.P47T >= -0.001].fillna(0)\n",
    "    train = train.merge(AGLO_rk[['AGLOMERADO', 'AGLO_rk']]).merge(Reg_rk[['Region', 'Reg_rk']])\n",
    "    train.to_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listo. Salvado el training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Borradores sobre los nombres de columnas.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Misma info, distinto nombre. \n",
    "# # Censo INDEC \n",
    "# md_1 = table[['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO', #las que no se erran, cant pers, sexo, edad, act, aglo\n",
    "#     'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "#       'P07', 'P08', 'P09', 'P10', 'P05']] #las x que buscan matches un poquito mas laxamente\n",
    "\n",
    "\n",
    "# #Mismas cosas, distinto nombre de columna para\n",
    "# # EPH INDEC\n",
    "# md_2 = EPH[['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "#     'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "#     'CH09','CH10','CH12','CH13','CH15']]\n",
    "\n",
    "# # # Controlar superposicion de columnas\n",
    "\n",
    "# # for i in range(len(md_1.columns))[:2]: \n",
    "# #     print('\\n')\n",
    "# #     for md in [md_1, md_2]:\n",
    "# #         col = md.columns[i]\n",
    "# #         print(col)\n",
    "# #         print(md[col].value_counts().sort_index()/len(md))\n",
    "\n",
    "# md_2.columns = md_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# # Agregar $$$. En millones de usd\n",
    "# # En millones de usd (USD = 30 ARS)\n",
    "# _USD = 30.5 #ARS\n",
    "# np.round(res_1.sum()/_USD/1e6, 1).sort_values().tail(14)\n",
    "\n",
    "# #PPALES\n",
    "# # negocio que no trabajo no laborable (V9_M)\n",
    "# # alquiler no laborable (V8_M)\n",
    "# # indemnizacion despido no laborable (V3_M)\n",
    "# # comision Ocupacion ppal (PP08F1)\n",
    "# # cuota alimentos no laborable (V12_M)\n",
    "# # subsidio ayuda social no laborable (V5_M)\n",
    "# # TOTAL otras ocupacions(TOT_P12)\n",
    "# # jubilacion no laborable (V2_M)\n",
    "# # TOTAL no laborables (T_VI)\n",
    "# # sueldo Ocupacion ppal(PP08D1)\n",
    "# # TOTAL Ocupacion ppal (P21)\n",
    "# # TOTAL TOTAL (P47T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Desoc, NA = 0. Not good.\n",
    "# variables = ['PP07J', #turno habitual\n",
    "#  'PP10D', #Desoc. Ha trabajado alguna vez?\n",
    "#  'PP10C', #Desoc. Hizo changa mientras buscaba?\n",
    "#  'PP07K', # Oc. ppal. Inc. serv. dom. Cobra con recibo\n",
    "#  'PP07G2', # Oc. ppal. Inc. serv. dom. aguinaldo\n",
    "#  'PP07G_59', # Oc. ppal. Inc. serv. dom. ninguno\n",
    "#  'PP07G3', # Oc. ppal. Inc. serv. dom. dias enfermedad\n",
    "#  'PP10E', # Desoc. Tiempo de que termino su ultimo trabajo/changa\n",
    "#  'PP07H', # Oc. ppal. Inc. serv. dom. descuento jubilatorio\n",
    "#  'PP07G4', # Oc. ppal. Inc. serv. dom. obra social\n",
    "#  'PP07I', # Oc. ppal. Inc. serv. dom. Aporta jub por sí mismo \n",
    "#  'PP07G1', # Oc. ppal. Inc. serv. dom. vacaciones pagas\n",
    "#  'CH07'] #Est civil\n",
    "\n",
    "# #en miles de USD\n",
    "# # res_byDPTO = res_DPTO.groupby(['DPTO', 'NOMDPTO', 'NOMPROV'])[variables].mean()\n",
    "# res_byDPTO = res_DPTO.groupby(['RADIO_REF_ID'])[variables].mean()\n",
    "\n",
    "\n",
    "# s = np.round(res_byDPTO, 2).sort_values(by = 'PP07K')#.head() \n",
    "# s.style.bar(color='#d65f5f')\n",
    "# #  'CAT_OCUP', #CAT_INAC\n",
    "# #  'CAT_INAC', #CAT_INAC\n",
    "\n",
    "# #  'CH08', obra social/salud. nums altos como para mean\n",
    "# # 'CH16', # Donde vivia hace 5. Desconfiable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# for col in result.columns:\n",
    "#     print('\\n')\n",
    "#     print(col)\n",
    "#     df_ = result.loc[result.P03 > 2]\n",
    "#     print(df_[col].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
