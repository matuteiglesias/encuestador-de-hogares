{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads EPH data, cleans it and arranges it to be used as 'training sets'. \n",
    "That is, for fitting any Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "startyr = 2018\n",
    "endyr = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "# radio_ref[['PROV','NOMPROV','DPTO', 'NOMDPTO']].drop_duplicates().to_csv('./../data/DPTO_PROV.csv', index = False)\n",
    "dpto_region = pd.read_csv('./../data/info/DPTO_PROV_Region.csv')\n",
    "radio_ref = radio_ref.merge(dpto_region)\n",
    "AGLO_Region = radio_ref[['AGLOMERADO', 'Region']].drop_duplicates()\n",
    "\n",
    "# Decision sobre cual es la region de un aglomerado. GBA tiene que ir a Gran Buenos Aires, aunque algunos de sus radios en partidos como Rodriguez, Escobar, etc sean region pampeana.\n",
    "# Viedma Patagones, se tendria que tirar de un lado, y la mayoria de sus radios, son Patagonia.\n",
    "# Se tiene que corregir a mano, porque el AGLO 0 SI tiene varias regiones.\n",
    "\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 33) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 93) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "\n",
    "### Match column names\n",
    "\n",
    "names_censo = ['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO',\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']\n",
    "\n",
    "\n",
    "names_EPH = ['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']\n",
    "\n",
    "col_mon = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar IPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimestral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_Q.csv'\n",
    "\n",
    "cpi = pd.read_csv(url, index_col = 0)\n",
    "cpi.index = pd.to_datetime(cpi.index)\n",
    "cpi = cpi['2003':]\n",
    "cpi.tail()\n",
    "\n",
    "## Forzar dia 15 del mes\n",
    "cpi.index = cpi.index - pd.offsets.MonthBegin(1) + pd.offsets.Day(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mensual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>1145.545470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-09-01</th>\n",
       "      <td>1205.458397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-01</th>\n",
       "      <td>1268.504816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-01</th>\n",
       "      <td>1334.848613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-12-01</th>\n",
       "      <td>1404.662242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  index\n",
       "2022-08-01  1145.545470\n",
       "2022-09-01  1205.458397\n",
       "2022-10-01  1268.504816\n",
       "2022-11-01  1334.848613\n",
       "2022-12-01  1404.662242"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_M.csv'\n",
    "\n",
    "cpi_M = pd.read_csv(url, index_col = 0)[['index']]\n",
    "cpi_M.index = pd.to_datetime(cpi_M.index)\n",
    "cpi_M = cpi_M['2003':]\n",
    "cpi_M.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencia de nivel de precios\n",
    "### 2016 - 01 - 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index    100.0\n",
       "Name: 2016-01-01 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_d.csv'\n",
    "\n",
    "cpi_d = pd.read_csv(url, index_col=0)[['index']]\n",
    "cpi_d.index = pd.to_datetime(cpi_d.index)\n",
    "cpi_d = cpi_d['2003':]\n",
    "\n",
    "cpi_d.loc['2016-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = cpi_d.loc['2016-01-01'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index    980.882535\n",
       "Name: 2022-05-01 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "mes_actual = datetime.today().replace(day=1).strftime(\"%Y-%m-%d\")\n",
    "cpi_M.loc[mes_actual]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPHs\n",
    "\n",
    "Los microdatos de la Encuesta Permanente de Hogares se pueden descargar con:\n",
    "\n",
    "``git clone https://github.com/matuteiglesias/microdatos-EPH-INDEC.git``\n",
    "\n",
    "(darle star al repositorio)\n",
    "\n",
    "tomando pull del mismo repositorio se va a poder actualizar con los nuevos microdatos a medida que se publican. \n",
    "\n",
    "``cd path/to/microdatos-EPH-INDEC``\n",
    "\n",
    "``git pull``\n",
    "\n",
    "\n",
    "Siempre y cuando el repositorio se mantenga actualizado. Y hasta que no se supere la capacidad de almacenamiento en repo github. \n",
    "\n",
    "El INDEC se toma aproximadamente 140 dias despues de terminado un trimestre (4 meses y medio) para subir las bases de microdatos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When running the first time we may not have the folder where training data is saved\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./../data/training/'):\n",
    "    os.makedirs('./../data/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGLO_rk = pd.read_csv('./../data/info/AGLO_rk')\n",
    "Reg_rk = pd.read_csv('./../data/info/Reg_rk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018\n",
      "18616\n",
      "18624\n",
      "18364\n",
      "18586\n",
      "(74151, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t318.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t118.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t218.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t418.txt\n",
      "(229794, 33)\n",
      "Hogar - Indiv merged:\n",
      "(234346, 48)\n",
      "No aglo agregado:\n",
      "(468692, 49)\n",
      "['2018-11-15T00:00:00.000000000' '2018-02-15T00:00:00.000000000'\n",
      " '2018-08-15T00:00:00.000000000' '2018-05-15T00:00:00.000000000']\n",
      "deflactado:\n",
      "(468692, 50)\n",
      "2019\n",
      "19035\n",
      "18610\n",
      "19251\n",
      "19338\n",
      "(76165, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t119.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t419.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t319.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t219.txt\n",
      "(234083, 33)\n",
      "Hogar - Indiv merged:\n",
      "(238760, 48)\n",
      "No aglo agregado:\n",
      "(477520, 49)\n",
      "['2019-11-15T00:00:00.000000000' '2019-08-15T00:00:00.000000000'\n",
      " '2019-05-15T00:00:00.000000000' '2019-02-15T00:00:00.000000000']\n",
      "deflactado:\n",
      "(477520, 50)\n",
      "2020\n",
      "16845\n",
      "11976\n",
      "14442\n",
      "13503\n",
      "(56724, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t220.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t120.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t420.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t320.txt\n",
      "(173960, 33)\n",
      "Hogar - Indiv merged:\n",
      "(178006, 48)\n",
      "No aglo agregado:\n",
      "(356012, 49)\n",
      "['2020-02-15T00:00:00.000000000' '2020-05-15T00:00:00.000000000'\n",
      " '2020-11-15T00:00:00.000000000' '2020-08-15T00:00:00.000000000']\n",
      "deflactado:\n",
      "(356012, 50)\n",
      "2021\n",
      "15423\n",
      "17037\n",
      "15620\n",
      "16285\n",
      "(64324, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t121.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t321.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t421.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t221.txt\n",
      "(192301, 33)\n",
      "Hogar - Indiv merged:\n",
      "(196227, 48)\n",
      "No aglo agregado:\n",
      "(392454, 49)\n",
      "['2021-02-15T00:00:00.000000000' '2021-11-15T00:00:00.000000000'\n",
      " '2021-05-15T00:00:00.000000000' '2021-08-15T00:00:00.000000000']\n",
      "deflactado:\n",
      "(392454, 50)\n"
     ]
    }
   ],
   "source": [
    "# from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "path ='./../../microdatos-EPH-INDEC/microdatos/' # depende de donde hayamos descargado los microdatos\n",
    "# path ='./../../EPH/microdatos/' # depende de donde hayamos descargado los microdatos\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../data/training/EPHARG_train_'+str(yr)+'.csv'\n",
    "    \n",
    "    if not os.path.exists(training_file): # Si todavia no existe la training data de ese anio.\n",
    "\n",
    "        allFiles = glob.glob(path + 'hogar/*'+str(yr)+'.txt')\n",
    "        frame = pd.DataFrame()\n",
    "        list_ = []\n",
    "        for file_ in allFiles:\n",
    "            df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                            usecols = ['CODUSU','ANO4','TRIMESTRE','IX_TOT', 'AGLOMERADO',\n",
    "            'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9']) \n",
    "#             ['II2', 'IV5', 'IX_TOT', 'II7', 'IV4', 'II1', 'IV7', 'IV6', 'IV11', 'IV8', 'IV3', 'II8', 'IV1', 'IV10', 'II9']\n",
    "\n",
    "            print(len(df))\n",
    "            list_ += [df]\n",
    "        df = pd.concat(list_)\n",
    "\n",
    "        # Correcciones Respuestas. Para que matchee censo\n",
    "        df = df.loc[df.IV1 != 9]\n",
    "        df['IV10'] = df['IV10'].map({1: 1, 2: 2, 3: 2, 0: 0, 9: 9})\n",
    "        df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "        df['II7'] = df['II7'].map({1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 6, 8: 6, 9: 6, 0: 0})\n",
    "        df['IX_TOT'] = df['IX_TOT'].clip(0, 8)\n",
    "\n",
    "        hogar = df\n",
    "        hogar = hogar.drop_duplicates()\n",
    "        print(hogar.shape)\n",
    "\n",
    "        allFiles = glob.glob(path + 'individual/usu_individual*'+str(yr)+'.txt')\n",
    "        frame = pd.DataFrame()\n",
    "        list_ = []\n",
    "        for file_ in allFiles:\n",
    "            print(file_)\n",
    "        #     print(file_)\n",
    "            df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                             usecols = ['CODUSU','ANO4','TRIMESTRE','CH04','CH06', 'AGLOMERADO', 'CH09','CH10','CH12','CH13','CH15'] +\\\n",
    "                             ['CH07', 'ESTADO','CAT_INAC','CAT_OCUP','PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K',\n",
    "                             'P47T', 'V3_M', 'T_VI', 'V12_M', 'TOT_P12', 'V5_M','V2_M', 'PP08D1', 'P21'])\n",
    "            df = df.rename(columns = {'ESTADO': 'CONDACT'})\n",
    "\n",
    "\n",
    "            list_ += [df]\n",
    "        df = pd.concat(list_)\n",
    "\n",
    "        # Correcciones Respuestas. Para que matchee censo\n",
    "        df['CH15'] = df['CH15'].map({1:1, 2:1, 3:1, 4:2, 5:2, 9:0})\n",
    "        df['CH06'] = df['CH06'].clip(0)\n",
    "        df['CH09'] = df['CH09'].map({1:1, 2:2, 0:2, 3:2})\n",
    "        df.loc[df['CH06'] < 14, 'CONDACT'] = 0 # Menores de 14 van con CONDACT 0, como en el Censo\n",
    "\n",
    "        ## En Censo, Jardin y educacion especial no preguntan terminado si/no.\n",
    "        df['CH12'] = df.CH12.replace(99, 0)\n",
    "        df.loc[df.CH12.isin([0, 1, 9]), 'CH13'] = 0\n",
    "\n",
    "    #     df['MAYOR'] = df['CH06'] >= 14 \n",
    "    #     df['MAYOR'] = df['CH06'] // 7\n",
    "    #     df['CONDACT'] = df['CAT_OCUP'].fillna(-1)\n",
    "\n",
    "        indiv = df\n",
    "        indiv = indiv.dropna(subset = ['P47T'])\n",
    "        print(indiv.shape)\n",
    "\n",
    "        indiv_table = indiv[list(indiv.columns.difference(hogar.columns)) + ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO']]\n",
    "        EPH = hogar.merge(indiv_table, on = ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO'])#, indicator = True)\n",
    "\n",
    "        print('Hogar - Indiv merged:')\n",
    "        print(EPH.shape)\n",
    "\n",
    "\n",
    "    #     EPH = EPH.loc[EPH.P47T != -9]\n",
    "\n",
    "        EPH = EPH.merge(AGLO_Region)\n",
    "\n",
    "        EPH_no_aglo = EPH.copy(); \n",
    "        EPH_no_aglo['AGLOMERADO'] = 0\n",
    "\n",
    "        EPH = pd.concat([EPH, EPH_no_aglo]).reset_index(drop = True)\n",
    "\n",
    "        print('No aglo agregado:')\n",
    "        print(EPH.shape)\n",
    "\n",
    "    #     # Quarters / deflation\n",
    "    #     EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "    #     EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') + MonthEnd(1)\n",
    "    # #     cpi_ultimo_Q = indice_precios['index'].values[-1]\n",
    "\n",
    "        # Quarters / deflation\n",
    "        EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "        EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') - pd.DateOffset(months=1) + pd.DateOffset(days=14)\n",
    "        print(EPH['Q'].unique())\n",
    "\n",
    "\n",
    "    #     EPH[col_mon] = cpi_mes_actual*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "        EPH[col_mon] = ix*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "\n",
    "        # 2018Q3 -> Mar19 1.3156\n",
    "        # 2018Q3 -> Abr19 1.361\n",
    "    #     EPH[col_mon] = 1.361*EPH[col_mon]\n",
    "\n",
    "        EPH[col_mon] = EPH[col_mon].round()\n",
    "\n",
    "        print('deflactado:')\n",
    "        print(EPH.shape)\n",
    "#         display(EPH[col_mon].mean())\n",
    "\n",
    "        training = EPH.rename(columns = dict(zip(names_EPH, names_censo)))\n",
    "        \n",
    "        # remove bad observations\n",
    "        training = training.loc[training.P47T >= -0.001].fillna(0)\n",
    "        \n",
    "        for col in ['CAT_OCUP', 'CH07', 'PP07G1', 'PP07G_59', 'PP07I', 'PP07J', 'PP07K']:\n",
    "            training = training.loc[training[col] != 9]\n",
    "        training = training.merge(AGLO_rk[['AGLOMERADO', 'AGLO_rk']]).merge(Reg_rk[['Region', 'Reg_rk']])\n",
    "        \n",
    "        ## Crear columnas binarias para ingreso.\n",
    "        training['INGRESO'] = (training.P47T > 100).astype(int)\n",
    "        training['INGRESO_NLB'] = (training.T_VI > 100).astype(int)\n",
    "        training['INGRESO_JUB'] = (training.V2_M > 100).astype(int)\n",
    "        training['INGRESO_SBS'] = (training.V5_M > 100).astype(int)\n",
    "        \n",
    "        ## Ordenar por id de hogar.\n",
    "        training = training.sort_values('CODUSU')\n",
    "        \n",
    "        training.to_csv(training_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de AGLOS y Regiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGLO_rk</th>\n",
       "      <th>AGLOMERADO</th>\n",
       "      <th>P47T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6270.023160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6385.555641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>7577.308184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7631.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>7763.054250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGLO_rk  AGLOMERADO         P47T\n",
       "0        0          18  6270.023160\n",
       "1        1          12  6385.555641\n",
       "2        2          29  7577.308184\n",
       "3        3           8  7631.666667\n",
       "4        4          23  7763.054250"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aglork = train_df.loc[train_df.CAT_OCUP == 3].groupby(['AGLOMERADO'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'AGLO_rk'})\n",
    "aglork.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGLO_rk = train_df.loc[(train_df.CAT_OCUP == 3) & (train_df.P47T >= 100)].groupby(['ANO4', 'AGLOMERADO'])[['P47T']].mean()\n",
    "AGLO_rk['AGLO_rk'] = AGLO_rk.rank(pct = True).round(3)\n",
    "AGLO_rk = AGLO_rk.sort_values('P47T').reset_index()\n",
    "\n",
    "Reg_rk = train_df.loc[(train_df.CAT_OCUP == 3) & (train_df.P47T >= 100)].groupby(['ANO4', 'Region'])[['P47T']].mean()\n",
    "Reg_rk['AGLO_rk'] = Reg_rk.rank(pct = True).round(3)\n",
    "Reg_rk = Reg_rk.sort_values('P47T').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ANO4</th>\n",
       "      <th>AGLOMERADO</th>\n",
       "      <th>AGLO_rk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>0.59375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>32</td>\n",
       "      <td>0.90625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2018</td>\n",
       "      <td>20</td>\n",
       "      <td>0.93750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2018</td>\n",
       "      <td>33</td>\n",
       "      <td>0.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2018</td>\n",
       "      <td>14</td>\n",
       "      <td>0.21875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2018</td>\n",
       "      <td>22</td>\n",
       "      <td>0.25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2018</td>\n",
       "      <td>26</td>\n",
       "      <td>0.46875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2018</td>\n",
       "      <td>30</td>\n",
       "      <td>0.71875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>2018</td>\n",
       "      <td>91</td>\n",
       "      <td>0.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2018</td>\n",
       "      <td>18</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2018</td>\n",
       "      <td>31</td>\n",
       "      <td>0.96875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2018</td>\n",
       "      <td>93</td>\n",
       "      <td>0.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2018</td>\n",
       "      <td>6</td>\n",
       "      <td>0.65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2018</td>\n",
       "      <td>38</td>\n",
       "      <td>0.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2018</td>\n",
       "      <td>36</td>\n",
       "      <td>0.37500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2018</td>\n",
       "      <td>29</td>\n",
       "      <td>0.15625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>2018</td>\n",
       "      <td>25</td>\n",
       "      <td>0.31250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2018</td>\n",
       "      <td>34</td>\n",
       "      <td>0.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2018</td>\n",
       "      <td>13</td>\n",
       "      <td>0.34375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>2018</td>\n",
       "      <td>15</td>\n",
       "      <td>0.09375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>0.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.12500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>0.03125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>2018</td>\n",
       "      <td>27</td>\n",
       "      <td>0.28125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2018</td>\n",
       "      <td>17</td>\n",
       "      <td>0.87500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>2018</td>\n",
       "      <td>23</td>\n",
       "      <td>0.06250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>2018</td>\n",
       "      <td>19</td>\n",
       "      <td>0.18750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>0.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1166</th>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.68750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ANO4  AGLOMERADO  AGLO_rk\n",
       "0     2018           0  0.59375\n",
       "1     2018          32  0.90625\n",
       "3     2018          10  0.43750\n",
       "7     2018          20  0.93750\n",
       "9     2018          33  0.56250\n",
       "12    2018           9  1.00000\n",
       "16    2018          14  0.21875\n",
       "24    2018          22  0.25000\n",
       "36    2018           2  0.75000\n",
       "45    2018          26  0.46875\n",
       "50    2018          30  0.71875\n",
       "60    2018          91  0.84375\n",
       "64    2018          18  0.00000\n",
       "88    2018          31  0.96875\n",
       "91    2018          93  0.81250\n",
       "94    2018           6  0.65625\n",
       "104   2018          38  0.53125\n",
       "119   2018          36  0.37500\n",
       "131   2018          29  0.15625\n",
       "166   2018          25  0.31250\n",
       "176   2018          34  0.62500\n",
       "188   2018          13  0.34375\n",
       "204   2018           7  0.50000\n",
       "213   2018          15  0.09375\n",
       "224   2018           3  0.78125\n",
       "292   2018          12  0.12500\n",
       "308   2018           8  0.03125\n",
       "373   2018          27  0.28125\n",
       "486   2018          17  0.87500\n",
       "520   2018          23  0.06250\n",
       "677   2018          19  0.18750\n",
       "867   2018           4  0.40625\n",
       "1166  2018           5  0.68750"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./../data/training/EPHARG_train_'+str(yr)+'.csv', \n",
    "            usecols = ['ANO4', 'AGLOMERADO', 'AGLO_rk']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_file = \n",
    "\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../data/training/EPHARG_train_'+str(yr)+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "for yr in [str(s) for s in range(2017, 2022)]:\n",
    "    print(yr)\n",
    "    train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')\n",
    "    train = train.loc[train.P47T >= -0.001].fillna(0)#.sample(400000)\n",
    "    df_list += [train]\n",
    "    \n",
    "train_df = pd.concat(df_list)\n",
    "\n",
    "AGLO_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['AGLOMERADO'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'AGLO_rk'})\n",
    "Reg_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['Region'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'Reg_rk'})\n",
    "\n",
    "AGLO_rk['AGLO_rk'] = AGLO_rk.AGLO_rk/AGLO_rk.AGLO_rk.max()\n",
    "AGLO_rk.to_csv('./../data/info/AGLO_rk', index = False)\n",
    "Reg_rk['Reg_rk'] = Reg_rk.Reg_rk/Reg_rk.Reg_rk.max()\n",
    "Reg_rk.to_csv('./../data/info/Reg_rk', index = False)\n",
    "\n",
    "# check it out\n",
    "# AGLO_rk.merge(pd.read_csv('./../data/info/aglo_labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGLO_rk = pd.read_csv('./../data/info/AGLO_rk')\n",
    "# Reg_rk = pd.read_csv('./../data/info/Reg_rk')\n",
    "\n",
    "# df_list = []\n",
    "# for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "# # for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "#     print(yr)\n",
    "#     train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')#.drop(['AGLO_rk', 'Reg_rk'], axis = 1)\n",
    "#     train = train.loc[train.P47T >= -0.001].fillna(0)\n",
    "#     train = train.merge(AGLO_rk[['AGLOMERADO', 'AGLO_rk']]).merge(Reg_rk[['Region', 'Reg_rk']])\n",
    "#     train.to_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listo. Salvado el training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
