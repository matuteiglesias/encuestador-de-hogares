{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads EPH data, cleans it and arranges it to be used as 'training sets'. \n",
    "That is, for fitting any Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTOS INTRODUCIDOS POR EL USUARIO\n",
      "Start year:  2020\n",
      "End year:  2021\n",
      "Overwrite:  True\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "if get_ipython() is None:\n",
    "    print('ARGUMENTOS TOMADOS DE CLI')\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='A script to process data for a range of years')\n",
    "\n",
    "    parser.add_argument('-y','--years', nargs='+', help='Set the range of years to process data for. Default is the current year and the next year', required=False, type=int, default=[2022, 2023])\n",
    "    parser.add_argument('-ow','--overwrite', nargs=1, required=False, default= True, help='Flag to specify if previous data should be overwritten. Default is True')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    overwrite = args.overwrite\n",
    "    startyr = args.years[0]\n",
    "    endyr = args.years[1]\n",
    "    \n",
    "else:\n",
    "    print('ARGUMENTOS INTRODUCIDOS POR EL USUARIO')\n",
    "    startyr = input(\"Enter the start year [default: 2022]: \") or 2022\n",
    "    endyr = input(\"Enter the end year [default: 2023]: \") or 2023\n",
    "    overwrite = input(\"Do you want to overwrite previous data? [y/n] [default: y]: \") or \"y\"\n",
    "\n",
    "    if overwrite.lower() == \"y\":\n",
    "        overwrite = True\n",
    "    else:\n",
    "        overwrite = False\n",
    "\n",
    "    #Convert the input to integers\n",
    "    startyr = int(startyr)\n",
    "    endyr = int(endyr)\n",
    "\n",
    "    print(\"Start year: \", startyr)\n",
    "    print(\"End year: \", endyr)\n",
    "    print(\"Overwrite: \", overwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "\n",
    "AGLO_Region = radio_ref[['AGLOMERADO', 'Region']].drop_duplicates()\n",
    "\n",
    "# Decision sobre cual es la region de un aglomerado. GBA tiene que ir a Gran Buenos Aires, aunque algunos de sus radios en partidos como Rodriguez, Escobar, etc sean region pampeana.\n",
    "# Viedma Patagones, se tendria que tirar de un lado, y la mayoria de sus radios, son Patagonia.\n",
    "# Se tiene que corregir a mano, porque el AGLO 0 SI tiene varias regiones.\n",
    "\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 33) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 93) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "\n",
    "### Match column names\n",
    "\n",
    "names_censo = ['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO',\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']\n",
    "\n",
    "\n",
    "names_EPH = ['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']\n",
    "\n",
    "col_mon = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar IPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimestral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_Q.csv'\n",
    "\n",
    "cpi = pd.read_csv(url, index_col = 0)\n",
    "cpi.index = pd.to_datetime(cpi.index)\n",
    "cpi = cpi['2003':]\n",
    "cpi.tail()\n",
    "\n",
    "## Forzar dia 15 del mes\n",
    "cpi.index = cpi.index - pd.offsets.MonthBegin(1) + pd.offsets.Day(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mensual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01</th>\n",
       "      <td>1565.973054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-01</th>\n",
       "      <td>1659.228084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01</th>\n",
       "      <td>1758.036531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-01</th>\n",
       "      <td>1862.729106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-01</th>\n",
       "      <td>1973.656212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  index\n",
       "2023-01-01  1565.973054\n",
       "2023-02-01  1659.228084\n",
       "2023-03-01  1758.036531\n",
       "2023-04-01  1862.729106\n",
       "2023-05-01  1973.656212"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_M.csv'\n",
    "\n",
    "cpi_M = pd.read_csv(url, index_col = 0)[['index']]\n",
    "cpi_M.index = pd.to_datetime(cpi_M.index)\n",
    "cpi_M = cpi_M['2003':]\n",
    "cpi_M.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencia de nivel de precios\n",
    "### 2016 - 01 - 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index    100.0\n",
       "Name: 2016-01-01 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_d.csv'\n",
    "\n",
    "cpi_d = pd.read_csv(url, index_col=0)[['index']]\n",
    "cpi_d.index = pd.to_datetime(cpi_d.index)\n",
    "cpi_d = cpi_d['2003':]\n",
    "\n",
    "cpi_d.loc['2016-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = cpi_d.loc['2016-01-01'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index    1565.973054\n",
       "Name: 2023-01-01 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "mes_actual = datetime.today().replace(day=1).strftime(\"%Y-%m-%d\")\n",
    "cpi_M.loc[mes_actual]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPHs\n",
    "\n",
    "Los microdatos de la Encuesta Permanente de Hogares (copias actualizadas de los archivos oficiales) estan disponibles en el repositorio:\n",
    "https://github.com/matuteiglesias/microdatos-EPH-INDEC.git\n",
    "\n",
    "\n",
    "SI ESTOS DATOS TE RESULTAN UTILES, TE PIDO DARLE UN STAR AL REPOSITORIO\n",
    "\n",
    "tomando pull del mismo repositorio se va a poder actualizar con los nuevos microdatos a medida que se publican. \n",
    "\n",
    "``cd path/to/microdatos-EPH-INDEC``\n",
    "\n",
    "``git pull``\n",
    "\n",
    "El INDEC se toma aproximadamente 130 dias luego de terminado un trimestre para subir las bases de microdatos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "import os\n",
    "\n",
    "# Verifico si existe el directorio donde se guardarian los microdatos\n",
    "directorio_microdatos = \"./../../microdatos-EPH-INDEC/\"\n",
    "\n",
    "# Si el directorio no existe, lo creo y clono el repositorio en ese lugar\n",
    "if not os.path.exists(directorio_microdatos):\n",
    "    os.makedirs(directorio_microdatos)\n",
    "\n",
    "    # Si el modulo git no esta instalado, lo instalo\n",
    "    try:\n",
    "        import git\n",
    "    except ImportError:\n",
    "        !pip install gitpython\n",
    "        import git\n",
    "\n",
    "    git.Repo.clone_from(\"https://github.com/matuteiglesias/microdatos-EPH-INDEC.git\", \n",
    "    directorio_microdatos)\n",
    "\n",
    "# Ahora, tenemos los microdatos de la EPH en el directorio ./../../microdatos-EPH-INDEC/microdatos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ademas, verifico si existe el directorio donde se guardarian los datos de entrenamiento\n",
    "if not os.path.exists('./../data/training/'):\n",
    "    os.makedirs('./../data/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "13503\n",
      "16845\n",
      "14442\n",
      "11976\n",
      "(56724, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t320.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t120.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t220.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t420.txt\n",
      "(173960, 33)\n",
      "Hogar - Indiv merged:\n",
      "(178006, 48)\n",
      "No aglo agregado:\n",
      "(356012, 49)\n",
      "['2020-08-15T00:00:00.000000000' '2020-02-15T00:00:00.000000000'\n",
      " '2020-11-15T00:00:00.000000000' '2020-05-15T00:00:00.000000000']\n",
      "deflactado:\n",
      "(356012, 50)\n"
     ]
    }
   ],
   "source": [
    "# from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../data/training/EPHARG_train_'+str(yr)+'.csv'\n",
    "    \n",
    "    # Si todavia no existe la training data de ese anio, o si la opcion overwrite esta activada:\n",
    "    if (not os.path.exists(training_file)) or (overwrite): \n",
    "\n",
    "        allFiles = glob.glob(directorio_microdatos + 'microdatos/hogar/*'+str(yr)+'.txt')\n",
    "        frame = pd.DataFrame()\n",
    "        list_ = []\n",
    "        for file_ in allFiles:\n",
    "            df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                            usecols = ['CODUSU','ANO4','TRIMESTRE','IX_TOT', 'AGLOMERADO',\n",
    "            'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9']) \n",
    "\n",
    "            print(len(df))\n",
    "            list_ += [df]\n",
    "        df = pd.concat(list_)\n",
    "\n",
    "        # Correcciones Respuestas. Para que matchee censo\n",
    "        df = df.loc[df.IV1 != 9]\n",
    "        df['IV10'] = df['IV10'].map({1: 1, 2: 2, 3: 2, 0: 0, 9: 9})\n",
    "        df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "        df['II7'] = df['II7'].map({1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 6, 8: 6, 9: 6, 0: 0})\n",
    "        df['IX_TOT'] = df['IX_TOT'].clip(0, 8)\n",
    "\n",
    "        hogar = df\n",
    "        hogar = hogar.drop_duplicates()\n",
    "        print(hogar.shape)\n",
    "\n",
    "        allFiles = glob.glob(directorio_microdatos + 'microdatos/individual/usu_individual*'+str(yr)+'.txt')\n",
    "        frame = pd.DataFrame()\n",
    "        list_ = []\n",
    "        for file_ in allFiles:\n",
    "            print(file_)\n",
    "        #     print(file_)\n",
    "            df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                             usecols = ['CODUSU','ANO4','TRIMESTRE','CH04','CH06', 'AGLOMERADO', 'CH09','CH10','CH12','CH13','CH15'] +\\\n",
    "                             ['CH07', 'ESTADO','CAT_INAC','CAT_OCUP','PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K',\n",
    "                             'P47T', 'V3_M', 'T_VI', 'V12_M', 'TOT_P12', 'V5_M','V2_M', 'PP08D1', 'P21'])\n",
    "            df = df.rename(columns = {'ESTADO': 'CONDACT'})\n",
    "\n",
    "\n",
    "            list_ += [df]\n",
    "        df = pd.concat(list_)\n",
    "\n",
    "        # Correcciones Respuestas. Para que matchee censo\n",
    "        df['CH15'] = df['CH15'].map({1:1, 2:1, 3:1, 4:2, 5:2, 9:0})\n",
    "        df['CH06'] = df['CH06'].clip(0)\n",
    "        df['CH09'] = df['CH09'].map({1:1, 2:2, 0:2, 3:2})\n",
    "        df.loc[df['CH06'] < 14, 'CONDACT'] = 0 # Menores de 14 van con CONDACT 0, como en el Censo\n",
    "\n",
    "        ## En Censo, Jardin y educacion especial no preguntan terminado si/no.\n",
    "        df['CH12'] = df.CH12.replace(99, 0)\n",
    "        df.loc[df.CH12.isin([0, 1, 9]), 'CH13'] = 0\n",
    "\n",
    "    #     df['MAYOR'] = df['CH06'] >= 14 \n",
    "    #     df['MAYOR'] = df['CH06'] // 7\n",
    "    #     df['CONDACT'] = df['CAT_OCUP'].fillna(-1)\n",
    "\n",
    "        indiv = df\n",
    "        indiv = indiv.dropna(subset = ['P47T'])\n",
    "        print(indiv.shape)\n",
    "\n",
    "        indiv_table = indiv[list(indiv.columns.difference(hogar.columns)) + ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO']]\n",
    "        EPH = hogar.merge(indiv_table, on = ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO'])#, indicator = True)\n",
    "\n",
    "        print('Hogar - Indiv merged:')\n",
    "        print(EPH.shape)\n",
    "\n",
    "\n",
    "    #     EPH = EPH.loc[EPH.P47T != -9]\n",
    "\n",
    "        EPH = EPH.merge(AGLO_Region)\n",
    "\n",
    "        EPH_no_aglo = EPH.copy(); \n",
    "        EPH_no_aglo['AGLOMERADO'] = 0\n",
    "\n",
    "        EPH = pd.concat([EPH, EPH_no_aglo]).reset_index(drop = True)\n",
    "\n",
    "        print('No aglo agregado:')\n",
    "        print(EPH.shape)\n",
    "\n",
    "    #     # Quarters / deflation\n",
    "    #     EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "    #     EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') + MonthEnd(1)\n",
    "    # #     cpi_ultimo_Q = indice_precios['index'].values[-1]\n",
    "\n",
    "        # Quarters / deflation\n",
    "        EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "        EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') - pd.DateOffset(months=1) + pd.DateOffset(days=14)\n",
    "        print(EPH['Q'].unique())\n",
    "\n",
    "\n",
    "    #     EPH[col_mon] = cpi_mes_actual*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "        EPH[col_mon] = ix*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "\n",
    "        # 2018Q3 -> Mar19 1.3156\n",
    "        # 2018Q3 -> Abr19 1.361\n",
    "    #     EPH[col_mon] = 1.361*EPH[col_mon]\n",
    "\n",
    "        EPH[col_mon] = EPH[col_mon].round()\n",
    "\n",
    "        print('deflactado:')\n",
    "        print(EPH.shape)\n",
    "#         display(EPH[col_mon].mean())\n",
    "\n",
    "        training = EPH.rename(columns = dict(zip(names_EPH, names_censo)))\n",
    "        \n",
    "        # remove bad observations\n",
    "        training = training.loc[training.P47T >= -0.001].fillna(0)\n",
    "        \n",
    "        for col in ['CAT_OCUP', 'CH07', 'PP07G1', 'PP07G_59', 'PP07I', 'PP07J', 'PP07K']:\n",
    "            training = training.loc[training[col] != 9]\n",
    "\n",
    "        ### RANKING AGLOMERADO\n",
    "        AGLO_rk = training.loc[(training.CAT_OCUP == 3) & (training.P47T >= 100)].groupby(['ANO4', 'AGLOMERADO'])[['P47T']].mean()\n",
    "        AGLO_rk['AGLO_rk'] = AGLO_rk.rank(pct = True).round(3)\n",
    "        AGLO_rk = AGLO_rk.sort_values('P47T').reset_index()\n",
    "\n",
    "        ### RANKING REGION\n",
    "        Reg_rk = training.loc[(training.CAT_OCUP == 3) & (training.P47T >= 100)].groupby(['ANO4', 'Region'])[['P47T']].mean()\n",
    "        Reg_rk['Reg_rk'] = Reg_rk.rank(pct = True).round(3)\n",
    "        Reg_rk = Reg_rk.sort_values('P47T').reset_index()\n",
    "            \n",
    "        training = training.merge(AGLO_rk[['ANO4', 'AGLOMERADO', 'AGLO_rk']]).merge(Reg_rk[['ANO4', 'Region', 'Reg_rk']])\n",
    "        \n",
    "        ## Crear columnas binarias para ingreso.\n",
    "        training['INGRESO'] = (training.P47T > 100).astype(int)\n",
    "        training['INGRESO_NLB'] = (training.T_VI > 100).astype(int)\n",
    "        training['INGRESO_JUB'] = (training.V2_M > 100).astype(int)\n",
    "        training['INGRESO_SBS'] = (training.V5_M > 100).astype(int)\n",
    "        \n",
    "        ## Ordenar por id de hogar.\n",
    "        training = training.sort_values('CODUSU')\n",
    "        \n",
    "        training.to_csv(training_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de AGLOS y Regiones\n",
    "\n",
    "A continuacion se extrae el ranking de aglomerados y regiones para cada uno de los anios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003\n",
      "2004\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     aglo_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(training_file, usecols \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mANO4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAGLOMERADO\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAGLO_rk\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mdrop_duplicates()\n\u001b[1;32m     11\u001b[0m     aglo_list \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [aglo_table]\n\u001b[0;32m---> 13\u001b[0m     regs_table \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(training_file, usecols \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mANO4\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mRegion\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mReg_rk\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39mdrop_duplicates()\n\u001b[1;32m     14\u001b[0m     regs_list \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [regs_table]\n\u001b[1;32m     16\u001b[0m aglo_rk \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(aglo_list)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "aglo_list = []\n",
    "regs_list = []\n",
    "\n",
    "startyr = 2003; endyr = 2023;\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../data/training/EPHARG_train_'+str(yr)+'.csv'\n",
    "    \n",
    "    aglo_table = pd.read_csv(training_file, usecols = ['ANO4', 'AGLOMERADO', 'AGLO_rk']).drop_duplicates()\n",
    "    aglo_list += [aglo_table]\n",
    "    \n",
    "    regs_table = pd.read_csv(training_file, usecols = ['ANO4', 'Region', 'Reg_rk']).drop_duplicates()\n",
    "    regs_list += [regs_table]\n",
    "    \n",
    "aglo_rk = pd.concat(aglo_list)\n",
    "regs_rk = pd.concat(regs_list)\n",
    "\n",
    "aglo_rk.to_csv('./../data/info/AGLO_rk', index = False)\n",
    "regs_rk.to_csv('./../data/info/Reg_rk', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listo. Salvado el training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
