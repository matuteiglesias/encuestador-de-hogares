{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera Datasets de entrenamiento a partir de los microdatos de EPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTOS INTRODUCIDOS POR EL USUARIO\n",
      "Start year:  2024\n",
      "End year:  2026\n",
      "Overwrite:  True\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "if get_ipython() is None:\n",
    "    print('ARGUMENTOS TOMADOS DE CLI')\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='A script to process data for a range of years')\n",
    "\n",
    "    parser.add_argument('-y','--years', nargs='+', help='Set the range of years to process data for. Default is the current year and the next year', required=False, type=int, default=[2022, 2023])\n",
    "    parser.add_argument('-ow','--overwrite', nargs=1, required=False, default= True, help='Flag to specify if previous data should be overwritten. Default is True')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    overwrite = args.overwrite\n",
    "    startyr = args.years[0]\n",
    "    endyr = args.years[1]\n",
    "    \n",
    "else:\n",
    "    print('ARGUMENTOS INTRODUCIDOS POR EL USUARIO')\n",
    "    startyr = input(\"Enter the start year [default: 2022]: \") or 2022\n",
    "    endyr = input(\"Enter the end year [default: 2023]: \") or 2023\n",
    "    overwrite = input(\"Do you want to overwrite previous data? [y/n] [default: y]: \") or \"y\"\n",
    "\n",
    "    if overwrite.lower() == \"y\":\n",
    "        overwrite = True\n",
    "    else:\n",
    "        overwrite = False\n",
    "\n",
    "    #Convert the input to integers\n",
    "    startyr = int(startyr)\n",
    "    endyr = int(endyr)\n",
    "\n",
    "    print(\"Start year: \", startyr)\n",
    "    print(\"End year: \", endyr)\n",
    "    print(\"Overwrite: \", overwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../../data/info/radio_ref.csv')\n",
    "\n",
    "AGLO_Region = radio_ref[['AGLOMERADO', 'Region']].drop_duplicates()\n",
    "\n",
    "# Decision sobre cual es la region de un aglomerado. GBA tiene que ir a Gran Buenos Aires, aunque algunos de sus radios en partidos como Rodriguez, Escobar, etc sean region pampeana.\n",
    "# Viedma Patagones, se tendria que tirar de un lado, y la mayoria de sus radios, son Patagonia.\n",
    "# Se tiene que corregir a mano, porque el AGLO 0 SI tiene varias regiones.\n",
    "\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 33) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 93) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "\n",
    "### Match column names\n",
    "\n",
    "names_censo = ['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO',\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']\n",
    "\n",
    "\n",
    "names_EPH = ['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']\n",
    "\n",
    "col_mon = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar IPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cpi_df(url: str, start_year: int, end_year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function creates a CPI dataframe from a given url and a specified range of years.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The url where the cpi data is located.\n",
    "    start_year (int): The first year to include in the dataframe.\n",
    "    end_year (int): The last year to include in the dataframe.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The created dataframe with the cpi data.\n",
    "    \"\"\"\n",
    "    cpi = pd.read_csv(url, index_col = 0) #reads csv from url and sets first column as index\n",
    "    cpi.index = pd.to_datetime(cpi.index) #convert index to datetime\n",
    "    cpi = cpi[str(start_year):str(end_year)] #filter dataframe by range of years\n",
    "    return cpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "ano_actual = datetime.today().strftime(\"%Y\")\n",
    "\n",
    "# Crear CPI dataframe, TRIMESTRAL\n",
    "cpi = create_cpi_df('https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_Q.csv', \n",
    "    2003, end_year=ano_actual)\n",
    "cpi.index = cpi.index - pd.offsets.MonthBegin(1) + pd.offsets.Day(14) #force day 15 of the month\n",
    "\n",
    "# Crear CPI dataframe, MENSUAL\n",
    "cpi_M = create_cpi_df('https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_M.csv', \n",
    "    2003, end_year=ano_actual)\n",
    "\n",
    "# Crear CPI dataframe, DIARIO\n",
    "cpi_d = create_cpi_df('https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_d.csv', \n",
    "    2003, end_year=ano_actual)\n",
    "\n",
    "# Fecha de referencia para el IPC. ix es el nivel del indice en la fecha de referencia, y es 100, por definicion\n",
    "ix = cpi_d.loc['2016-01-01']['index']\n",
    "\n",
    "# Primer dia del mes en curso\n",
    "mes_actual = datetime.today().replace(day=1).strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPHs\n",
    "\n",
    "Los microdatos de la Encuesta Permanente de Hogares (copias actualizadas de los archivos oficiales) estan disponibles en el repositorio:\n",
    "https://github.com/matuteiglesias/microdatos-EPH-INDEC.git\n",
    "\n",
    "\n",
    "SI ESTOS DATOS TE RESULTAN UTILES, TE PIDO DARLE UN STAR AL REPOSITORIO\n",
    "\n",
    "tomando pull del mismo repositorio se va a poder actualizar con los nuevos microdatos a medida que se publican. \n",
    "\n",
    "``cd path/to/microdatos-EPH-INDEC``\n",
    "\n",
    "``git pull``\n",
    "\n",
    "El INDEC se toma aproximadamente 130 dias luego de terminado un trimestre para subir las bases de microdatos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "import os\n",
    "\n",
    "# Verifico si existe el directorio donde se guardarian los microdatos\n",
    "# directorio_microdatos = \"./../../microdatos-EPH-INDEC/\"\n",
    "directorio_microdatos = \"/home/matias/repos/microdatos-EPH-INDEC/\"\n",
    "\n",
    "# Si el directorio no existe, lo creo y clono el repositorio en ese lugar\n",
    "if not os.path.exists(directorio_microdatos):\n",
    "    os.makedirs(directorio_microdatos)\n",
    "\n",
    "    # Si el modulo git no esta instalado, lo instalo\n",
    "    try:\n",
    "        import git\n",
    "    except ImportError:\n",
    "        !pip install gitpython\n",
    "        import git\n",
    "\n",
    "    git.Repo.clone_from(\"https://github.com/matuteiglesias/microdatos-EPH-INDEC.git\", \n",
    "    directorio_microdatos)\n",
    "\n",
    "# Ahora, tenemos los microdatos de la EPH en el directorio ./../../microdatos-EPH-INDEC/microdatos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ademas, verifico si existe el directorio donde se guardarian los datos de entrenamiento\n",
    "if not os.path.exists('./../data/training/'):\n",
    "    os.makedirs('./../data/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# df = concatenate_files(2022, '/path/to/directory', ['column1', 'column2', 'column3'])\n",
    "def read_data_from_files(year: int, directory: str, columns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a year, a directory, and a list of columns, this function concatenates all the files in the directory that match the year and returns a DataFrame containing only the specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "        - year (int): The year to match in the file names.\n",
    "        - directory (str): The directory where the files are located.\n",
    "        - columns (List[str]): The list of columns to keep in the returned DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        - pd.DataFrame: A DataFrame containing only the specified columns from all the files in the directory that match the year.\n",
    "    \"\"\"\n",
    "    print('looking for: ', directory + '/*{}.txt'.format(str(year)[2:]))\n",
    "    all_files = glob.glob(directory + '/*{}.txt'.format(str(year)[2:]))\n",
    "    list_ = []\n",
    "    for file_ in all_files:\n",
    "        # Read the file and select only the specified columns\n",
    "        df = pd.read_csv(file_, index_col=None, header=0, delimiter=';', usecols=columns)\n",
    "        # Add the selected columns of the file to the list\n",
    "        list_ += [df]\n",
    "    # Concatenate all the selected columns of the files and return as a DataFrame\n",
    "    return pd.concat(list_)\n",
    "\n",
    "\n",
    "def correct_responses_hogar(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies corrections to the input DataFrame to match the census.\n",
    "    \"\"\"\n",
    "    df = df.loc[df.IV1 != 9]\n",
    "    df['IV10'] = df['IV10'].map({1: 1, 2: 2, 3: 2, 0: 0, 9: 9})\n",
    "    df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "    df['II7'] = df['II7'].map({1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 6, 8: 6, 9: 6, 0: 0})\n",
    "\n",
    "    df['IX_TOT'] = df['IX_TOT'].clip(0, 8)\n",
    "    return df\n",
    "\n",
    "def correct_responses_individual(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply cleaning steps to the input DataFrame. (Para que matchee censo)\n",
    "    :param df: DataFrame to be cleaned.\n",
    "    :return: A copy of the input DataFrame with the cleaning steps applied.\n",
    "    \"\"\"\n",
    "    # Copy the input DataFrame to prevent modifying the original object\n",
    "    df = df_.copy()\n",
    "\n",
    "    df['CH15'] = df['CH15'].map({1:1, 2:1, 3:1, 4:2, 5:2, 9:0})\n",
    "    df['CH06'] = df['CH06'].clip(0)\n",
    "    df['CH09'] = df['CH09'].map({1:1, 2:2, 0:2, 3:2})\n",
    "    df.loc[df['CH06'] < 14, 'CONDACT'] = 0 # Menores de 14 van con CONDACT 0, como en el Censo\n",
    "\n",
    "    df = df.rename(columns = {'ESTADO': 'CONDACT'})\n",
    "\n",
    "    ## En Censo, Jardin y educacion especial no preguntan terminado si/no.\n",
    "    df['CH12'] = df.CH12.replace(99, 0)\n",
    "    df.loc[df.CH12.isin([0, 1, 9]), 'CH13'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024\n",
      "looking for:  /home/matias/repos/microdatos-EPH-INDEC/microdatos/hogar/*24.txt\n",
      "looking for:  /home/matias/repos/microdatos-EPH-INDEC/microdatos/individual/*24.txt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['V2_M', 'V5_M']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m hogar_df \u001b[38;5;241m=\u001b[39m hogar_df\u001b[38;5;241m.\u001b[39mdrop_duplicates()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m## Data Individual\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m individual_df \u001b[38;5;241m=\u001b[39m \u001b[43mread_data_from_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectorio_microdatos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmicrodatos/individual\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCODUSU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mANO4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTRIMESTRE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCH04\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCH06\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAGLOMERADO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCH09\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCH10\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCH12\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCH13\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCH15\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCH07\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mESTADO\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCAT_INAC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCAT_OCUP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07G1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07G2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07G3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07G4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07G_59\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07H\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07I\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07J\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP07K\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP47T\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mV3_M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT_VI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mV12_M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTOT_P12\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mV5_M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mV2_M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPP08D1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mP21\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m individual_df \u001b[38;5;241m=\u001b[39m correct_responses_individual(individual_df)\n\u001b[1;32m     25\u001b[0m individual_df \u001b[38;5;241m=\u001b[39m individual_df\u001b[38;5;241m.\u001b[39mdropna(subset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP47T\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m, in \u001b[0;36mread_data_from_files\u001b[0;34m(year, directory, columns)\u001b[0m\n\u001b[1;32m     19\u001b[0m list_ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_ \u001b[38;5;129;01min\u001b[39;00m all_files:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Read the file and select only the specified columns\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Add the selected columns of the file to the list\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     list_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [df]\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[1;32m    139\u001b[0m ):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_usecols_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/new_env/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:988\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    986\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m     )\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['V2_M', 'V5_M']"
     ]
    }
   ],
   "source": [
    "# from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../../data/training/EPHARG_train_{}.csv'.format(yr)\n",
    "    \n",
    "    # Si todavia no existe la training data de ese anio, o si la opcion overwrite esta activada:\n",
    "    if (not os.path.exists(training_file)) or (overwrite): \n",
    "\n",
    "        ## Data Hogares\n",
    "        hogar_df = read_data_from_files(y, directorio_microdatos + 'microdatos/hogar',\n",
    "            ['CODUSU','ANO4','TRIMESTRE','IX_TOT', 'AGLOMERADO', 'IV1', 'IV3', 'IV4','IV5',\n",
    "            'IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9'])\n",
    "        hogar_df = correct_responses_hogar(hogar_df)\n",
    "        hogar_df = hogar_df.drop_duplicates()\n",
    "\n",
    "\n",
    "        ## Data Individual\n",
    "        individual_df = read_data_from_files(y, directorio_microdatos + 'microdatos/individual',\n",
    "            ['CODUSU','ANO4','TRIMESTRE','CH04','CH06', 'AGLOMERADO', 'CH09','CH10','CH12','CH13','CH15',\n",
    "            'CH07', 'ESTADO','CAT_INAC','CAT_OCUP','PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K',\n",
    "            'P47T', 'V3_M', 'T_VI', 'V12_M', 'TOT_P12', 'V5_M','V2_M', 'PP08D1', 'P21'])\n",
    "        individual_df = correct_responses_individual(individual_df)\n",
    "\n",
    "        individual_df = individual_df.dropna(subset = ['P47T'])\n",
    "\n",
    "        indiv_table = individual_df[list(individual_df.columns.difference(hogar_df.columns)) + ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO']]\n",
    "\n",
    "        EPH = hogar_df.merge(indiv_table, on = ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO'])#, indicator = True)\n",
    "\n",
    "        EPH = EPH.merge(AGLO_Region)\n",
    "\n",
    "        EPH_no_aglo = EPH.copy(); \n",
    "        EPH_no_aglo['AGLOMERADO'] = 0\n",
    "\n",
    "        EPH = pd.concat([EPH, EPH_no_aglo]).reset_index(drop = True)\n",
    "\n",
    "        # Quarters / deflation\n",
    "        EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "        EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') - pd.DateOffset(months=1) + pd.DateOffset(days=14)\n",
    "\n",
    "        EPH[col_mon] = ix*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "\n",
    "        EPH[col_mon] = EPH[col_mon].round()\n",
    "\n",
    "        training = EPH.rename(columns = dict(zip(names_EPH, names_censo)))\n",
    "        \n",
    "        # remove bad observations\n",
    "        training = training.loc[training.P47T >= -0.001].fillna(0)\n",
    "        \n",
    "        for col in ['CAT_OCUP', 'CH07', 'PP07G1', 'PP07G_59', 'PP07I', 'PP07J', 'PP07K']:\n",
    "            training = training.loc[training[col] != 9]\n",
    "\n",
    "        ### RANKING AGLOMERADO\n",
    "        AGLO_rk = training.loc[(training.CAT_OCUP == 3) & (training.P47T >= 100)].groupby(['ANO4', 'AGLOMERADO'])[['P47T']].mean()\n",
    "        AGLO_rk['AGLO_rk'] = AGLO_rk.rank(pct = True).round(3)\n",
    "        AGLO_rk = AGLO_rk.sort_values('P47T').reset_index()\n",
    "        AGLO_rk = AGLO_rk[['ANO4', 'AGLOMERADO', 'AGLO_rk']].drop_duplicates()\n",
    "\n",
    "        ### RANKING REGION\n",
    "        Reg_rk = training.loc[(training.CAT_OCUP == 3) & (training.P47T >= 100)].groupby(['ANO4', 'Region'])[['P47T']].mean()\n",
    "        Reg_rk['Reg_rk'] = Reg_rk.rank(pct = True).round(3)\n",
    "        Reg_rk = Reg_rk.sort_values('P47T').reset_index()\n",
    "        Reg_rk = Reg_rk[['ANO4', 'Region', 'Reg_rk']].drop_duplicates()\n",
    "            \n",
    "        training = training.merge(AGLO_rk).merge(Reg_rk)\n",
    "        \n",
    "        ## Crear columnas binarias para ingreso.\n",
    "        training['INGRESO'] = (training.P47T > 100).astype(int)\n",
    "        training['INGRESO_NLB'] = (training.T_VI > 100).astype(int)\n",
    "        training['INGRESO_JUB'] = (training.V2_M > 100).astype(int)\n",
    "        training['INGRESO_SBS'] = (training.V5_M > 100).astype(int)\n",
    "        \n",
    "        ## Ordenar por id de hogar.\n",
    "        training = training.sort_values('CODUSU')\n",
    "        \n",
    "        training.to_csv(training_file, index = False)\n",
    "        print('Saved to', training_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de AGLOS y Regiones\n",
    "\n",
    "A continuacion se extrae el ranking de aglomerados y regiones para cada uno de los anios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "aglo_list = []\n",
    "regs_list = []\n",
    "\n",
    "startyr = 2003; endyr = 2025;\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../../data/training/EPHARG_train_'+str(yr)+'.csv'\n",
    "    \n",
    "    aglo_table = pd.read_csv(training_file, usecols = ['ANO4', 'AGLOMERADO', 'AGLO_rk']).drop_duplicates()\n",
    "    aglo_list += [aglo_table]\n",
    "    \n",
    "    regs_table = pd.read_csv(training_file, usecols = ['ANO4', 'Region', 'Reg_rk']).drop_duplicates()\n",
    "    regs_list += [regs_table]\n",
    "    \n",
    "aglo_rk = pd.concat(aglo_list)\n",
    "regs_rk = pd.concat(regs_list)\n",
    "\n",
    "aglo_rk.to_csv('./../../data/info/AGLO_rk', index = False)\n",
    "regs_rk.to_csv('./../../data/info/Reg_rk', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listo. Salvado el training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
