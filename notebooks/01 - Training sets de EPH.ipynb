{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genera Datasets de entrenamiento a partir de los microdatos de EPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARGUMENTOS INTRODUCIDOS POR EL USUARIO\n",
      "Start year:  2015\n",
      "End year:  2016\n",
      "Overwrite:  True\n"
     ]
    }
   ],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "if get_ipython() is None:\n",
    "    print('ARGUMENTOS TOMADOS DE CLI')\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='A script to process data for a range of years')\n",
    "\n",
    "    parser.add_argument('-y','--years', nargs='+', help='Set the range of years to process data for. Default is the current year and the next year', required=False, type=int, default=[2022, 2023])\n",
    "    parser.add_argument('-ow','--overwrite', nargs=1, required=False, default= True, help='Flag to specify if previous data should be overwritten. Default is True')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    overwrite = args.overwrite\n",
    "    startyr = args.years[0]\n",
    "    endyr = args.years[1]\n",
    "    \n",
    "else:\n",
    "    print('ARGUMENTOS INTRODUCIDOS POR EL USUARIO')\n",
    "    startyr = input(\"Enter the start year [default: 2022]: \") or 2022\n",
    "    endyr = input(\"Enter the end year [default: 2023]: \") or 2023\n",
    "    overwrite = input(\"Do you want to overwrite previous data? [y/n] [default: y]: \") or \"y\"\n",
    "\n",
    "    if overwrite.lower() == \"y\":\n",
    "        overwrite = True\n",
    "    else:\n",
    "        overwrite = False\n",
    "\n",
    "    #Convert the input to integers\n",
    "    startyr = int(startyr)\n",
    "    endyr = int(endyr)\n",
    "\n",
    "    print(\"Start year: \", startyr)\n",
    "    print(\"End year: \", endyr)\n",
    "    print(\"Overwrite: \", overwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "\n",
    "AGLO_Region = radio_ref[['AGLOMERADO', 'Region']].drop_duplicates()\n",
    "\n",
    "# Decision sobre cual es la region de un aglomerado. GBA tiene que ir a Gran Buenos Aires, aunque algunos de sus radios en partidos como Rodriguez, Escobar, etc sean region pampeana.\n",
    "# Viedma Patagones, se tendria que tirar de un lado, y la mayoria de sus radios, son Patagonia.\n",
    "# Se tiene que corregir a mano, porque el AGLO 0 SI tiene varias regiones.\n",
    "\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 33) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 93) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "\n",
    "### Match column names\n",
    "\n",
    "names_censo = ['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO',\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']\n",
    "\n",
    "\n",
    "names_EPH = ['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']\n",
    "\n",
    "col_mon = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar IPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cpi_df(url: str, start_year: int, end_year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function creates a CPI dataframe from a given url and a specified range of years.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The url where the cpi data is located.\n",
    "    start_year (int): The first year to include in the dataframe.\n",
    "    end_year (int): The last year to include in the dataframe.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The created dataframe with the cpi data.\n",
    "    \"\"\"\n",
    "    cpi = pd.read_csv(url, index_col = 0) #reads csv from url and sets first column as index\n",
    "    cpi.index = pd.to_datetime(cpi.index) #convert index to datetime\n",
    "    cpi = cpi[str(start_year):str(end_year)] #filter dataframe by range of years\n",
    "    return cpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_index            1.194752\n",
       "index             1565.973054\n",
       "log_index_diff       0.025106\n",
       "pct_m                5.955085\n",
       "Name: 2023-01-01 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "ano_actual = datetime.today().strftime(\"%Y\")\n",
    "\n",
    "# Crear CPI dataframe, TRIMESTRAL\n",
    "cpi = create_cpi_df('https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_Q.csv', \n",
    "    2003, end_year=ano_actual)\n",
    "cpi.index = cpi.index - pd.offsets.MonthBegin(1) + pd.offsets.Day(14) #force day 15 of the month\n",
    "\n",
    "# Crear CPI dataframe, MENSUAL\n",
    "cpi_M = create_cpi_df('https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_M.csv', \n",
    "    2003, end_year=ano_actual)\n",
    "\n",
    "# Crear CPI dataframe, DIARIO\n",
    "cpi_d = create_cpi_df('https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_d.csv', \n",
    "    2003, end_year=ano_actual)\n",
    "\n",
    "# Fecha de referencia para el IPC. ix es el nivel del indice en la fecha de referencia, y es 100, por definicion\n",
    "ix = cpi_d.loc['2016-01-01']['index']\n",
    "\n",
    "# Primer dia del mes en curso\n",
    "mes_actual = datetime.today().replace(day=1).strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPHs\n",
    "\n",
    "Los microdatos de la Encuesta Permanente de Hogares (copias actualizadas de los archivos oficiales) estan disponibles en el repositorio:\n",
    "https://github.com/matuteiglesias/microdatos-EPH-INDEC.git\n",
    "\n",
    "\n",
    "SI ESTOS DATOS TE RESULTAN UTILES, TE PIDO DARLE UN STAR AL REPOSITORIO\n",
    "\n",
    "tomando pull del mismo repositorio se va a poder actualizar con los nuevos microdatos a medida que se publican. \n",
    "\n",
    "``cd path/to/microdatos-EPH-INDEC``\n",
    "\n",
    "``git pull``\n",
    "\n",
    "El INDEC se toma aproximadamente 130 dias luego de terminado un trimestre para subir las bases de microdatos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "import os\n",
    "\n",
    "# Verifico si existe el directorio donde se guardarian los microdatos\n",
    "directorio_microdatos = \"./../../microdatos-EPH-INDEC/\"\n",
    "\n",
    "# Si el directorio no existe, lo creo y clono el repositorio en ese lugar\n",
    "if not os.path.exists(directorio_microdatos):\n",
    "    os.makedirs(directorio_microdatos)\n",
    "\n",
    "    # Si el modulo git no esta instalado, lo instalo\n",
    "    try:\n",
    "        import git\n",
    "    except ImportError:\n",
    "        !pip install gitpython\n",
    "        import git\n",
    "\n",
    "    git.Repo.clone_from(\"https://github.com/matuteiglesias/microdatos-EPH-INDEC.git\", \n",
    "    directorio_microdatos)\n",
    "\n",
    "# Ahora, tenemos los microdatos de la EPH en el directorio ./../../microdatos-EPH-INDEC/microdatos/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ademas, verifico si existe el directorio donde se guardarian los datos de entrenamiento\n",
    "if not os.path.exists('./../data/training/'):\n",
    "    os.makedirs('./../data/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# df = concatenate_files(2022, '/path/to/directory', ['column1', 'column2', 'column3'])\n",
    "def read_data_from_files(year: int, directory: str, columns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a year, a directory, and a list of columns, this function concatenates all the files in the directory that match the year and returns a DataFrame containing only the specified columns.\n",
    "    \n",
    "    Parameters:\n",
    "        - year (int): The year to match in the file names.\n",
    "        - directory (str): The directory where the files are located.\n",
    "        - columns (List[str]): The list of columns to keep in the returned DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        - pd.DataFrame: A DataFrame containing only the specified columns from all the files in the directory that match the year.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(directory + '/*{}.txt'.format(str(year)[2:]))\n",
    "    list_ = []\n",
    "    for file_ in all_files:\n",
    "        # Read the file and select only the specified columns\n",
    "        df = pd.read_csv(file_, index_col=None, header=0, delimiter=';', usecols=columns)\n",
    "        # Add the selected columns of the file to the list\n",
    "        list_ += [df]\n",
    "    # Concatenate all the selected columns of the files and return as a DataFrame\n",
    "    return pd.concat(list_)\n",
    "\n",
    "\n",
    "def correct_responses_hogar(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies corrections to the input DataFrame to match the census.\n",
    "    \"\"\"\n",
    "    df = df.loc[df.IV1 != 9]\n",
    "    df['IV10'] = df['IV10'].map({1: 1, 2: 2, 3: 2, 0: 0, 9: 9})\n",
    "    df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "    df['II7'] = df['II7'].map({1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 6, 8: 6, 9: 6, 0: 0})\n",
    "\n",
    "    df['IX_TOT'] = df['IX_TOT'].clip(0, 8)\n",
    "    return df\n",
    "\n",
    "def correct_responses_individual(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply cleaning steps to the input DataFrame. (Para que matchee censo)\n",
    "    :param df: DataFrame to be cleaned.\n",
    "    :return: A copy of the input DataFrame with the cleaning steps applied.\n",
    "    \"\"\"\n",
    "    # Copy the input DataFrame to prevent modifying the original object\n",
    "    df = df_.copy()\n",
    "\n",
    "    df['CH15'] = df['CH15'].map({1:1, 2:1, 3:1, 4:2, 5:2, 9:0})\n",
    "    df['CH06'] = df['CH06'].clip(0)\n",
    "    df['CH09'] = df['CH09'].map({1:1, 2:2, 0:2, 3:2})\n",
    "    df.loc[df['CH06'] < 14, 'CONDACT'] = 0 # Menores de 14 van con CONDACT 0, como en el Censo\n",
    "\n",
    "    df = df.rename(columns = {'ESTADO': 'CONDACT'})\n",
    "\n",
    "    ## En Censo, Jardin y educacion especial no preguntan terminado si/no.\n",
    "    df['CH12'] = df.CH12.replace(99, 0)\n",
    "    df.loc[df.CH12.isin([0, 1, 9]), 'CH13'] = 0\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015\n",
      "Hogar - Indiv merged:\n",
      "Saved to ./../data/training/EPHARG_train_15.csv\n"
     ]
    }
   ],
   "source": [
    "# from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../data/training/EPHARG_train_{}.csv'.format(yr)\n",
    "    \n",
    "    # Si todavia no existe la training data de ese anio, o si la opcion overwrite esta activada:\n",
    "    if (not os.path.exists(training_file)) or (overwrite): \n",
    "\n",
    "        ## Data Hogares\n",
    "        hogar_df = read_data_from_files(y, directorio_microdatos + 'microdatos/hogar',\n",
    "            ['CODUSU','ANO4','TRIMESTRE','IX_TOT', 'AGLOMERADO', 'IV1', 'IV3', 'IV4','IV5',\n",
    "            'IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9'])\n",
    "        hogar_df = correct_responses_hogar(hogar_df)\n",
    "        hogar_df = hogar_df.drop_duplicates()\n",
    "\n",
    "        ## Data Individual\n",
    "        individual_df = read_data_from_files(y, directorio_microdatos + 'microdatos/individual',\n",
    "            ['CODUSU','ANO4','TRIMESTRE','CH04','CH06', 'AGLOMERADO', 'CH09','CH10','CH12','CH13','CH15',\n",
    "            'CH07', 'ESTADO','CAT_INAC','CAT_OCUP','PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K',\n",
    "            'P47T', 'V3_M', 'T_VI', 'V12_M', 'TOT_P12', 'V5_M','V2_M', 'PP08D1', 'P21'])\n",
    "        individual_df = correct_responses_individual(individual_df)\n",
    "\n",
    "        individual_df = individual_df.dropna(subset = ['P47T'])\n",
    "\n",
    "        indiv_table = individual_df[list(individual_df.columns.difference(hogar_df.columns)) + ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO']]\n",
    "\n",
    "        EPH = hogar_df.merge(indiv_table, on = ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO'])#, indicator = True)\n",
    "\n",
    "        EPH = EPH.merge(AGLO_Region)\n",
    "\n",
    "        EPH_no_aglo = EPH.copy(); \n",
    "        EPH_no_aglo['AGLOMERADO'] = 0\n",
    "\n",
    "        EPH = pd.concat([EPH, EPH_no_aglo]).reset_index(drop = True)\n",
    "\n",
    "        # Quarters / deflation\n",
    "        EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "        EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') - pd.DateOffset(months=1) + pd.DateOffset(days=14)\n",
    "\n",
    "        EPH[col_mon] = ix*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "\n",
    "        EPH[col_mon] = EPH[col_mon].round()\n",
    "\n",
    "        training = EPH.rename(columns = dict(zip(names_EPH, names_censo)))\n",
    "        \n",
    "        # remove bad observations\n",
    "        training = training.loc[training.P47T >= -0.001].fillna(0)\n",
    "        \n",
    "        for col in ['CAT_OCUP', 'CH07', 'PP07G1', 'PP07G_59', 'PP07I', 'PP07J', 'PP07K']:\n",
    "            training = training.loc[training[col] != 9]\n",
    "\n",
    "        ### RANKING AGLOMERADO\n",
    "        AGLO_rk = training.loc[(training.CAT_OCUP == 3) & (training.P47T >= 100)].groupby(['ANO4', 'AGLOMERADO'])[['P47T']].mean()\n",
    "        AGLO_rk['AGLO_rk'] = AGLO_rk.rank(pct = True).round(3)\n",
    "        AGLO_rk = AGLO_rk.sort_values('P47T').reset_index()\n",
    "        AGLO_rk = AGLO_rk[['ANO4', 'AGLOMERADO', 'AGLO_rk']].drop_duplicates()\n",
    "\n",
    "        ### RANKING REGION\n",
    "        Reg_rk = training.loc[(training.CAT_OCUP == 3) & (training.P47T >= 100)].groupby(['ANO4', 'Region'])[['P47T']].mean()\n",
    "        Reg_rk['Reg_rk'] = Reg_rk.rank(pct = True).round(3)\n",
    "        Reg_rk = Reg_rk.sort_values('P47T').reset_index()\n",
    "        Reg_rk = Reg_rk[['ANO4', 'Region', 'Reg_rk']].drop_duplicates()\n",
    "            \n",
    "        training = training.merge(AGLO_rk).merge(Reg_rk)\n",
    "        \n",
    "        ## Crear columnas binarias para ingreso.\n",
    "        training['INGRESO'] = (training.P47T > 100).astype(int)\n",
    "        training['INGRESO_NLB'] = (training.T_VI > 100).astype(int)\n",
    "        training['INGRESO_JUB'] = (training.V2_M > 100).astype(int)\n",
    "        training['INGRESO_SBS'] = (training.V5_M > 100).astype(int)\n",
    "        \n",
    "        ## Ordenar por id de hogar.\n",
    "        training = training.sort_values('CODUSU')\n",
    "        \n",
    "        training.to_csv(training_file, index = False)\n",
    "        print('Saved to', training_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de AGLOS y Regiones\n",
    "\n",
    "A continuacion se extrae el ranking de aglomerados y regiones para cada uno de los anios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    }
   ],
   "source": [
    "aglo_list = []\n",
    "regs_list = []\n",
    "\n",
    "startyr = 2023; endyr = 2024;\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../data/training/EPHARG_train_'+str(yr)+'.csv'\n",
    "    \n",
    "    aglo_table = pd.read_csv(training_file, usecols = ['ANO4', 'AGLOMERADO', 'AGLO_rk']).drop_duplicates()\n",
    "    aglo_list += [aglo_table]\n",
    "    \n",
    "    regs_table = pd.read_csv(training_file, usecols = ['ANO4', 'Region', 'Reg_rk']).drop_duplicates()\n",
    "    regs_list += [regs_table]\n",
    "    \n",
    "aglo_rk = pd.concat(aglo_list)\n",
    "regs_rk = pd.concat(regs_list)\n",
    "\n",
    "aglo_rk.to_csv('./../data/info/AGLO_rk', index = False)\n",
    "regs_rk.to_csv('./../data/info/Reg_rk', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listo. Salvado el training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
