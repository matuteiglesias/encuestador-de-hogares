{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads EPH data, cleans it and arranges it to be used as 'training sets'. \n",
    "That is, for fitting any Machine Learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startyr = 2020\n",
    "endyr = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "radio_ref = pd.read_csv('./../data/info/radio_ref.csv')\n",
    "# radio_ref[['PROV','NOMPROV','DPTO', 'NOMDPTO']].drop_duplicates().to_csv('./../data/DPTO_PROV.csv', index = False)\n",
    "dpto_region = pd.read_csv('./../data/info/DPTO_PROV_Region.csv')\n",
    "radio_ref = radio_ref.merge(dpto_region)\n",
    "AGLO_Region = radio_ref[['AGLOMERADO', 'Region']].drop_duplicates()\n",
    "\n",
    "# Decision sobre cual es la region de un aglomerado. GBA tiene que ir a Gran Buenos Aires, aunque algunos de sus radios en partidos como Rodriguez, Escobar, etc sean region pampeana.\n",
    "# Viedma Patagones, se tendria que tirar de un lado, y la mayoria de sus radios, son Patagonia.\n",
    "# Se tiene que corregir a mano, porque el AGLO 0 SI tiene varias regiones.\n",
    "\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 33) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "AGLO_Region = AGLO_Region.loc[~((AGLO_Region.AGLOMERADO == 93) & (AGLO_Region.Region == 'Pampeana'))]\n",
    "\n",
    "### Match column names\n",
    "\n",
    "names_censo = ['IX_TOT', 'P02', 'P03', 'CONDACT', 'AGLOMERADO',\n",
    "    'V01', 'H05', 'H06', 'H07', 'H08', 'H09', 'H10', 'H11', 'H12', 'H16', 'H15', 'PROP', 'H14', 'H13',\n",
    "      'P07', 'P08', 'P09', 'P10', 'P05']\n",
    "\n",
    "\n",
    "names_EPH = ['IX_TOT','CH04','CH06','CONDACT', 'AGLOMERADO',\n",
    "    'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9',\n",
    "    'CH09','CH10','CH12','CH13','CH15']\n",
    "\n",
    "col_mon = [u'P21', u'P47T', u'PP08D1', u'TOT_P12', u'T_VI', u'V12_M', u'V2_M', u'V3_M', u'V5_M']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar IPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trimestral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-02-15</th>\n",
       "      <td>543.660505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-15</th>\n",
       "      <td>607.262336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-14</th>\n",
       "      <td>661.074581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-15</th>\n",
       "      <td>722.013731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-15</th>\n",
       "      <td>793.945664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index\n",
       "Q                     \n",
       "2021-02-15  543.660505\n",
       "2021-05-15  607.262336\n",
       "2021-08-14  661.074581\n",
       "2021-11-15  722.013731\n",
       "2022-02-15  793.945664"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_Q.csv'\n",
    "\n",
    "cpi = pd.read_csv(url, index_col = 0)\n",
    "cpi.index = pd.to_datetime(cpi.index)\n",
    "cpi = cpi['2003':]\n",
    "cpi.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mensual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-11-01</th>\n",
       "      <td>721.772599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-01</th>\n",
       "      <td>744.987237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>768.948535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>793.680510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-01</th>\n",
       "      <td>819.207948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 index\n",
       "2021-11-01  721.772599\n",
       "2021-12-01  744.987237\n",
       "2022-01-01  768.948535\n",
       "2022-02-01  793.680510\n",
       "2022-03-01  819.207948"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_M.csv'\n",
    "\n",
    "cpi_M = pd.read_csv(url, index_col = 0)[['index']]\n",
    "cpi_M.index = pd.to_datetime(cpi_M.index)\n",
    "cpi_M = cpi_M['2003':]\n",
    "cpi_M.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencia de nivel de precios\n",
    "### 2016 - 01 - 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index    100.0\n",
       "Name: 2016-01-01 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/matuteiglesias/IPC-Argentina/main/data/info/indice_precios_d.csv'\n",
    "\n",
    "cpi_d = pd.read_csv(url, index_col=0)[['index']]\n",
    "cpi_d.index = pd.to_datetime(cpi_d.index)\n",
    "cpi_d = cpi_d['2003':]\n",
    "\n",
    "cpi_d.loc['2016-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = cpi_d.loc['2016-01-01'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index    768.948535\n",
       "Name: 2022-01-01 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "mes_actual = datetime.today().replace(day=1).strftime(\"%Y-%m-%d\")\n",
    "cpi_M.loc[mes_actual]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar EPHs\n",
    "\n",
    "Los microdatos de la Encuesta Permanente de Hogares se pueden descargar con:\n",
    "\n",
    "``git clone https://github.com/matuteiglesias/microdatos-EPH-INDEC.git``\n",
    "\n",
    "(darle star al repositorio)\n",
    "\n",
    "tomando pull del mismo repositorio se va a poder actualizar con los nuevos microdatos a medida que se publican. \n",
    "\n",
    "``cd path/to/microdatos-EPH-INDEC``\n",
    "\n",
    "``git pull``\n",
    "\n",
    "\n",
    "Siempre y cuando el repositorio se mantenga actualizado. Y hasta que no se supere la capacidad de almacenamiento en repo github. \n",
    "\n",
    "El INDEC se toma aproximadamente 140 dias despues de terminado un trimestre (4 meses y medio) para subir las bases de microdatos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When running the first time we may not have the folder where training data is saved\n",
    "import os\n",
    "\n",
    "if not os.path.exists('./../data/training/'):\n",
    "    os.makedirs('./../data/training/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGLO_rk = pd.read_csv('./../data/info/AGLO_rk')\n",
    "Reg_rk = pd.read_csv('./../data/info/Reg_rk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "2021\n",
      "15423\n",
      "15620\n",
      "(31021, 19)\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t121.txt\n",
      "./../../microdatos-EPH-INDEC/microdatos/individual/usu_individual_t221.txt\n",
      "(93615, 33)\n",
      "Hogar - Indiv merged:\n",
      "(95762, 49)\n",
      "No aglo agregado:\n",
      "(191524, 50)\n",
      "['2021-02-15T00:00:00.000000000' '2021-05-15T00:00:00.000000000']\n",
      "deflactado:\n",
      "(191524, 51)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "P21        2153.600677\n",
       "P47T       3445.085525\n",
       "PP08D1     4132.536329\n",
       "TOT_P12     122.826017\n",
       "T_VI       1088.979888\n",
       "V12_M        60.977538\n",
       "V2_M        823.149976\n",
       "V3_M         13.422788\n",
       "V5_M         84.366951\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "fill value must be in categories",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-925d6ec022c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_EPH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames_censo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP47T\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAGLO_rk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AGLOMERADO'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AGLO_rk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReg_rk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Region'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Reg_rk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   4315\u001b[0m         \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4316\u001b[0m     ) -> Optional[\"DataFrame\"]:\n\u001b[0;32m-> 4317\u001b[0;31m         return super().fillna(\n\u001b[0m\u001b[1;32m   4318\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4319\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   6076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6077\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6078\u001b[0;31m                 new_data = self._mgr.fillna(\n\u001b[0m\u001b[1;32m   6079\u001b[0m                     \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6080\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"BlockManager\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         return self.apply(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0;34m\"fillna\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdowncast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, limit, inplace, downcast)\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdowncast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1778\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1779\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1780\u001b[0m         return [\n\u001b[1;32m   1781\u001b[0m             self.make_block_same_class(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/arrays/categorical.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, limit)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fill value must be in categories\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: fill value must be in categories"
     ]
    }
   ],
   "source": [
    "# from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "path ='./../../microdatos-EPH-INDEC/microdatos/' # depende de donde hayamos descargado los microdatos\n",
    "# path ='./../../EPH/microdatos/' # depende de donde hayamos descargado los microdatos\n",
    "\n",
    "for y in range(startyr, endyr):\n",
    "    print(y)\n",
    "    yr = str(y)[2:]\n",
    "    training_file = './../data/training/EPHARG_train_'+str(yr)+'.csv'\n",
    "    \n",
    "    if not os.path.exists(training_file): # Si todavia no existe la training data de ese anio.\n",
    "\n",
    "        allFiles = glob.glob(path + 'hogar/*'+str(yr)+'.txt')\n",
    "        frame = pd.DataFrame()\n",
    "        list_ = []\n",
    "        for file_ in allFiles:\n",
    "            df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                            usecols = ['CODUSU','ANO4','TRIMESTRE','IX_TOT', 'AGLOMERADO',\n",
    "            'IV1', 'IV3', 'IV4','IV5','IV6','IV7','IV8','IV10','IV11','II1','II2','II7','II8','II9']) \n",
    "#             ['II2', 'IV5', 'IX_TOT', 'II7', 'IV4', 'II1', 'IV7', 'IV6', 'IV11', 'IV8', 'IV3', 'II8', 'IV1', 'IV10', 'II9']\n",
    "\n",
    "            print(len(df))\n",
    "            list_ += [df]\n",
    "        df = pd.concat(list_)\n",
    "\n",
    "        # Correcciones Respuestas. Para que matchee censo\n",
    "        df = df.loc[df.IV1 != 9]\n",
    "        df['IV10'] = df['IV10'].map({1: 1, 2: 2, 3: 2, 0: 0, 9: 9})\n",
    "        df['II9'] = df['II9'].map({1: 1, 2: 2, 3: 2, 4: 4, 0: 0})\n",
    "        df['II7'] = df['II7'].map({1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 6, 8: 6, 9: 6, 0: 0})\n",
    "        df['IX_TOT'] = df['IX_TOT'].clip(0, 8)\n",
    "\n",
    "        hogar = df\n",
    "        hogar = hogar.drop_duplicates()\n",
    "        print(hogar.shape)\n",
    "\n",
    "        allFiles = glob.glob(path + 'individual/usu_individual*'+str(yr)+'.txt')\n",
    "        frame = pd.DataFrame()\n",
    "        list_ = []\n",
    "        for file_ in allFiles:\n",
    "            print(file_)\n",
    "        #     print(file_)\n",
    "            df = pd.read_csv(file_,index_col=None, header=0, delimiter = ';',\n",
    "                             usecols = ['CODUSU','ANO4','TRIMESTRE','CH04','CH06', 'AGLOMERADO', 'CH09','CH10','CH12','CH13','CH15'] +\\\n",
    "                             ['CH07', 'ESTADO','CAT_INAC','CAT_OCUP','PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K',\n",
    "                             'P47T', 'V3_M', 'T_VI', 'V12_M', 'TOT_P12', 'V5_M','V2_M', 'PP08D1', 'P21'])\n",
    "            df = df.rename(columns = {'ESTADO': 'CONDACT'})\n",
    "\n",
    "\n",
    "            list_ += [df]\n",
    "        df = pd.concat(list_)\n",
    "\n",
    "        # Correcciones Respuestas. Para que matchee censo\n",
    "        df['CH15'] = df['CH15'].map({1:1, 2:1, 3:1, 4:2, 5:2, 9:0})\n",
    "        df['CH06'] = df['CH06'].clip(0)\n",
    "        df['CH09'] = df['CH09'].map({1:1, 2:2, 0:2, 3:2})\n",
    "        df.loc[df['CH06'] < 14, 'CONDACT'] = 0 # Menores de 14 van con CONDACT 0, como en el Censo\n",
    "\n",
    "        ## En Censo, Jardin y educacion especial no preguntan terminado si/no.\n",
    "        df['CH12'] = df.CH12.replace(99, 0)\n",
    "        df.loc[df.CH12.isin([0, 1, 9]), 'CH13'] = 0\n",
    "\n",
    "    #     df['MAYOR'] = df['CH06'] >= 14 \n",
    "    #     df['MAYOR'] = df['CH06'] // 7\n",
    "    #     df['CONDACT'] = df['CAT_OCUP'].fillna(-1)\n",
    "\n",
    "        indiv = df\n",
    "        indiv = indiv.dropna(subset = ['P47T'])\n",
    "        print(indiv.shape)\n",
    "\n",
    "        indiv_table = indiv[list(indiv.columns.difference(hogar.columns)) + ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO']]\n",
    "        EPH = hogar.merge(indiv_table, on = ['CODUSU', 'ANO4', 'TRIMESTRE', 'AGLOMERADO'], indicator = True)\n",
    "\n",
    "        print('Hogar - Indiv merged:')\n",
    "        print(EPH.shape)\n",
    "\n",
    "\n",
    "    #     EPH = EPH.loc[EPH.P47T != -9]\n",
    "\n",
    "        EPH = EPH.merge(AGLO_Region)\n",
    "\n",
    "        EPH_no_aglo = EPH.copy(); \n",
    "        EPH_no_aglo['AGLOMERADO'] = 0\n",
    "\n",
    "        EPH = pd.concat([EPH, EPH_no_aglo]).reset_index(drop = True)\n",
    "\n",
    "        print('No aglo agregado:')\n",
    "        print(EPH.shape)\n",
    "\n",
    "    #     # Quarters / deflation\n",
    "    #     EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "    #     EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') + MonthEnd(1)\n",
    "    # #     cpi_ultimo_Q = indice_precios['index'].values[-1]\n",
    "\n",
    "        # Quarters / deflation\n",
    "        EPH['Q'] = EPH.ANO4.astype(str) + ':' + (3*EPH.TRIMESTRE).astype(str)\n",
    "        EPH['Q'] = pd.to_datetime(EPH['Q'], format='%Y:%m') - pd.DateOffset(months=1) + pd.DateOffset(days=14)\n",
    "        print(EPH['Q'].unique())\n",
    "\n",
    "\n",
    "    #     EPH[col_mon] = cpi_mes_actual*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "        EPH[col_mon] = ix*EPH[col_mon].div(EPH[['Q'] + col_mon].merge(cpi, on = 'Q', how = 'left')['index'].values, 0)\n",
    "\n",
    "        # 2018Q3 -> Mar19 1.3156\n",
    "        # 2018Q3 -> Abr19 1.361\n",
    "    #     EPH[col_mon] = 1.361*EPH[col_mon]\n",
    "\n",
    "        EPH[col_mon] = EPH[col_mon].round()\n",
    "\n",
    "        print('deflactado:')\n",
    "        print(EPH.shape)\n",
    "#         display(EPH[col_mon].mean())\n",
    "\n",
    "        training = EPH.rename(columns = dict(zip(names_EPH, names_censo)))\n",
    "        \n",
    "        training = training.loc[training.P47T >= -0.001].fillna(0)\n",
    "        training = training.merge(AGLO_rk[['AGLOMERADO', 'AGLO_rk']]).merge(Reg_rk[['Region', 'Reg_rk']])\n",
    "        \n",
    "        ## Crear columnas binarias para ingreso.\n",
    "        training['INGRESO'] = (training.P47T > 100).astype(int)\n",
    "        training['INGRESO_NLB'] = (training.T_VI > 100).astype(int)\n",
    "        training['INGRESO_JUB'] = (training.V2_M > 100).astype(int)\n",
    "        training['INGRESO_SBS'] = (training.V5_M > 100).astype(int)\n",
    "        \n",
    "        ## Ordenar por id de hogar.\n",
    "        training = training.sort_values('CODUSU')\n",
    "        \n",
    "        training.to_csv(training_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking de AGLOS y Regiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = []\n",
    "# for yr in [str(s) for s in [2006, 2011, 2016]]:\n",
    "# # for yr in [str(s) for s in [2018, 2020]]:\n",
    "# # for yr in [str(s) for s in [2020]]:  # Esto depende de los anios en que tengamos la microdata, podemos elegir\n",
    "# # for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "#     print(yr)\n",
    "#     train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')\n",
    "#     train = train.loc[train.P47T >= -0.001].fillna(0)#.sample(400000)\n",
    "#     df_list += [train]\n",
    "    \n",
    "# train_df = pd.concat(df_list)\n",
    "\n",
    "# AGLO_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['AGLOMERADO'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'AGLO_rk'})\n",
    "# Reg_rk = train_df.loc[train_df.CAT_OCUP == 3].groupby(['Region'])[['P47T']].mean().sort_values('P47T').reset_index().reset_index().rename(columns = {'index':'Reg_rk'})\n",
    "\n",
    "# AGLO_rk['AGLO_rk'] = AGLO_rk.AGLO_rk/AGLO_rk.AGLO_rk.max()\n",
    "# AGLO_rk.to_csv('./../data/info/AGLO_rk', index = False)\n",
    "# Reg_rk['Reg_rk'] = Reg_rk.Reg_rk/Reg_rk.Reg_rk.max()\n",
    "# Reg_rk.to_csv('./../data/info/Reg_rk', index = False)\n",
    "\n",
    "# # check it out\n",
    "# # AGLO_rk.merge(pd.read_csv('./../data/info/aglo_labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGLO_rk = pd.read_csv('./../data/info/AGLO_rk')\n",
    "# Reg_rk = pd.read_csv('./../data/info/Reg_rk')\n",
    "\n",
    "# df_list = []\n",
    "# for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "# # for yr in [str(s) for s in range(startyr, endyr)]:\n",
    "#     print(yr)\n",
    "#     train = pd.read_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv')#.drop(['AGLO_rk', 'Reg_rk'], axis = 1)\n",
    "#     train = train.loc[train.P47T >= -0.001].fillna(0)\n",
    "#     train = train.merge(AGLO_rk[['AGLOMERADO', 'AGLO_rk']]).merge(Reg_rk[['Region', 'Reg_rk']])\n",
    "#     train.to_csv('./../data/training/EPHARG_train_'+yr[2:]+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listo. Salvado el training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
